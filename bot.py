import os
import json
import time
import re
import requests
import httpx
import math
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from openai import OpenAI

from telegram import (
    Update,
    ReplyKeyboardMarkup,
    InlineKeyboardButton,
    InlineKeyboardMarkup,
)

from datetime import datetime, timedelta, date

from telegram.ext import (
    ApplicationBuilder,
    CommandHandler,
    MessageHandler,
    CallbackQueryHandler,
    ContextTypes,
    filters,
)

import gspread
from oauth2client.service_account import ServiceAccountCredentials

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOKEN = os.getenv("BOT_TOKEN")
APP_URL = (os.getenv("APP_URL") or "").strip()
CHANNEL_ID = (os.getenv("CHANNEL_ID") or "").strip()  # ì˜ˆ: @ì±„ë„ì•„ì´ë”” ë˜ëŠ” -100xxxxxxxxxxxx

# ğŸ”´ ì—¬ê¸°ë§Œ ë„¤ ë´‡ ìœ ì €ë„¤ì„ìœ¼ë¡œ ìˆ˜ì •í•˜ë©´ ë¨ (@ ë¹¼ê³ )
BOT_USERNAME = "castlive_bot"  # ì˜ˆ: @castlive_bot ì´ë¼ë©´ "castlive_bot"

# ğŸ”¹ Gemini API í‚¤ (í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()

# ğŸ”¹ ê´€ë¦¬ì ID ëª©ë¡ (ì‰¼í‘œë¡œ ì—¬ëŸ¬ ëª… ê°€ëŠ¥) ì˜ˆ: "123456789,987654321"
_admin_ids_raw = os.getenv("ADMIN_IDS", "")
ADMIN_IDS = [
    int(x.strip())
    for x in _admin_ids_raw.split(",")
    if x.strip().isdigit()
]


def is_admin(update: Update) -> bool:
    """ì´ ëª…ë ¹ì–´ë¥¼ ëˆ„ê°€ í˜¸ì¶œí–ˆëŠ”ì§€ í™•ì¸í•´ì„œ, ê´€ë¦¬ìë©´ True ë¦¬í„´"""
    if not ADMIN_IDS:
        # ADMIN_IDSë¥¼ ì•ˆ ë„£ì—ˆìœ¼ë©´ ê·¸ëƒ¥ ëª¨ë‘ í—ˆìš© (í…ŒìŠ¤íŠ¸ìš©)
        return True
    user = update.effective_user
    return bool(user and user.id in ADMIN_IDS)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë‚ ì§œ í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_kst_now() -> datetime:
    """í•œêµ­ ì‹œê°„ ê¸°ì¤€ í˜„ì¬ ì‹œê° (UTC+9)"""
    return datetime.utcnow() + timedelta(hours=9)


def get_date_labels():
    """
    ì˜¤ëŠ˜ / ë‚´ì¼ ë‚ ì§œë¥¼ 'M.DD' í˜•ì‹ìœ¼ë¡œ ëŒë ¤ì¤Œ
    ì˜ˆ: ( '11.14', '11.15' )
    """
    now_kst = get_kst_now().date()
    today = now_kst
    tomorrow = now_kst + timedelta(days=1)

    today_str = f"{today.month}.{today.day:02d}"
    tomorrow_str = f"{tomorrow.month}.{tomorrow.day:02d}"
    return today_str, tomorrow_str

def get_tomorrow_mmdd_str() -> str:
    """
    mazgtv í…Œì´ë¸”ì˜ '11-28 (ê¸ˆ) 02:45' ê°™ì€ ë‚ ì§œì—ì„œ
    ì•ë¶€ë¶„ 'MM-DD' ì™€ ë¹„êµí•˜ê¸° ìœ„í•œ ë‚´ì¼ ë‚ ì§œ ë¬¸ìì—´ ìƒì„± (ì˜ˆ: '11-28')
    """
    tomorrow = get_kst_now().date() + timedelta(days=1)
    return f"{tomorrow.month:02d}-{tomorrow.day:02d}"

def get_tomorrow_keywords():
    """
    í•´ì™¸ë¶„ì„ ë¦¬ìŠ¤íŠ¸ì—ì„œ 'ë‚´ì¼ ê²½ê¸°'ë§Œ í•„í„°ë§í•˜ê¸° ìœ„í•œ í‚¤ì›Œë“œ ì„¸íŠ¸ ìƒì„±.
    - 'ë‚´ì¼'
    - 11.28 / 11-28 / 11/28 ê°™ì€ ì—¬ëŸ¬ ë‚ ì§œ í¬ë§·
    """
    tomorrow = get_kst_now().date() + timedelta(days=1)
    m = tomorrow.month
    d = tomorrow.day

    md_dot_1 = f"{m}.{d}"
    md_dot_2 = f"{m}.{d:02d}"
    md_dash_1 = f"{m}-{d}"
    md_dash_2 = f"{m}-{d:02d}"
    md_slash_1 = f"{m}/{d}"
    md_slash_2 = f"{m}/{d:02d}"

    return {
        "ë‚´ì¼",
        md_dot_1, md_dot_2,
        md_dash_1, md_dash_2,
        md_slash_1, md_slash_2,
    }


def get_menu_caption() -> str:
    """ë©”ì¸ ë©”ë‰´ ì„¤ëª… í…ìŠ¤íŠ¸ (ì˜¤ëŠ˜/ë‚´ì¼ ë‚ ì§œ ìë™ ë°˜ì˜)"""
    today_str, tomorrow_str = get_date_labels()
    return (
        "ğŸ“Œ ìŠ¤í¬ì¸  ì •ë³´&ë¶„ì„ ê³µìœ ë°© ë©”ë‰´ ì•ˆë‚´\n\n"
        "1ï¸âƒ£ ì‹¤ì‹œê°„ ë¬´ë£Œ ì¤‘ê³„ - GOAT-TV ë¼ì´ë¸Œ ì¤‘ê³„ ë°”ë¡œê°€ê¸°\n"
        f"2ï¸âƒ£ {today_str} ê²½ê¸° ë¶„ì„í”½ - ì¢…ëª©ë³„ë¡œ {today_str} ê²½ê¸° ë¶„ì„ì„ í™•ì¸í•˜ì„¸ìš”\n"
        f"3ï¸âƒ£ {tomorrow_str} ê²½ê¸° ë¶„ì„í”½ - ì¢…ëª©ë³„ë¡œ {tomorrow_str} ê²½ê¸° ë¶„ì„ì„ í™•ì¸í•˜ì„¸ìš”\n"
        "4ï¸âƒ£ ìŠ¤í¬ì¸  ë‰´ìŠ¤ ìš”ì•½ - ì£¼ìš” ì´ìŠˆ & ë‰´ìŠ¤ ìš”ì•½ ì •ë¦¬\n\n"
        "ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì›í•˜ëŠ” ë©”ë‰´ë¥¼ ì„ íƒí•˜ì„¸ìš” ğŸ‘‡"
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¶„ì„/ë‰´ìŠ¤ ë°ì´í„° (ì˜ˆì‹œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ANALYSIS_TODAY = {
    "ì¶•êµ¬": [],
    "ë†êµ¬": [],
    "ì•¼êµ¬": [],
    "ë°°êµ¬": [],
}
ANALYSIS_TOMORROW = {
    "ì¶•êµ¬": [],
    "ë†êµ¬": [],
    "ì•¼êµ¬": [],
    "ë°°êµ¬": [],
}

ANALYSIS_DATA_MAP = {
    "today": ANALYSIS_TODAY,
    "tomorrow": ANALYSIS_TOMORROW,
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë‹¤ìŒ ìŠ¤í¬ì¸  ì¹´í…Œê³ ë¦¬ ID ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DevTools > Network ì—ì„œ harmony contents.json ìš”ì²­ í™•ì¸ í›„
# defaultCategoryId3 ì˜ value ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ì„¸íŒ….
DAUM_CATEGORY_IDS = {
    # í•´ì™¸ì¶•êµ¬
    "world_soccer": os.getenv("DAUM_CAT_WORLD_SOCCER", "100032"),

    # êµ­ë‚´ì¶•êµ¬ (Kë¦¬ê·¸)
    "soccer_kleague": os.getenv("DAUM_CAT_SOCCER_KLEAGUE", "1027"),

    # êµ­ë‚´ì•¼êµ¬ (KBO)
    "baseball_kbo": os.getenv("DAUM_CAT_BASEBALL_KBO", "1028"),

    # í•´ì™¸ì•¼êµ¬ (MLB)
    "baseball_world": os.getenv("DAUM_CAT_BASEBALL_WORLD", "1015"),

    # ë†êµ¬
    "basketball": os.getenv("DAUM_CAT_BASKETBALL", "1029"),

    # ë°°êµ¬
    "volleyball": os.getenv("DAUM_CAT_VOLLEYBALL", "100033"),
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ êµ¬ê¸€ ì‹œíŠ¸ ì—°ë™ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_gs_client = None  # gspread í´ë¼ì´ì–¸íŠ¸ ìºì‹œìš©


def get_gs_client():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„œë¹„ìŠ¤ê³„ì • JSON ì½ì–´ì„œ gspread í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    global _gs_client
    if _gs_client is not None:
        return _gs_client

    key_raw = os.getenv("GOOGLE_SERVICE_KEY")
    if not key_raw:
        print("[GSHEET] GOOGLE_SERVICE_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹œíŠ¸ ì—°ë™ ê±´ë„ˆëœ€.")
        return None

    try:
        key_data = json.loads(key_raw)
    except Exception as e:
        print(f"[GSHEET] GOOGLE_SERVICE_KEY JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None

    scope = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive",
    ]

    creds = ServiceAccountCredentials.from_json_keyfile_dict(key_data, scope)
    _gs_client = gspread.authorize(creds)
    print("[GSHEET] gspread ì¸ì¦ ì™„ë£Œ")
    return _gs_client


def summarize_text(text: str, max_len: int = 400) -> str:
    """
    (ì˜ˆì „ìš©) ì•„ì£¼ ë‹¨ìˆœí•œ ìš”ì•½: ë¬¸ì¥ì„ ì˜ë¼ì„œ ì•ì—ì„œë¶€í„° max_lenê¹Œì§€ ìë¥´ëŠ” ë°©ì‹.
    """
    text = text.replace("\n", " ").strip()
    sentences = re.split(r'(?<=[\.!?ë‹¤ìš”])\s+', text)
    result = ""
    for s in sentences:
        s = s.strip()
        if not s:
            continue
        if not result:
            candidate = s
        else:
            candidate = result + " " + s
        if len(candidate) > max_len:
            break
        result = candidate
    if not result:
        result = text[:max_len]
    return result


def clean_daum_body_text(text: str) -> str:
    """
    ë‹¤ìŒ ë‰´ìŠ¤ ë³¸ë¬¸ì—ì„œ ë²ˆì—­/ìš”ì•½ UI, ì–¸ì–´ ëª©ë¡, ê¸°ì í¬ë ˆë”§/ì‚¬ì§„ ì„¤ëª… ë“±
    ë¶ˆí•„ìš”í•œ ë¬¸ì¥ì„ ìµœëŒ€í•œ ì œê±°í•˜ê³  ê¸°ì‚¬ ë³¸ë¬¸ë§Œ ë‚¨ê¸´ë‹¤.
    """
    if not text:
        return ""

    # 1ë‹¨ê³„: ì¤„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , ë¹ˆ ì¤„ ì œê±°
    lines = [l.strip() for l in text.splitlines() if l.strip()]

    blacklist = [
        "ìŒì„±ìœ¼ë¡œ ë“£ê¸°",
        "ìŒì„± ì¬ìƒ",
        "ìŒì„±ì¬ìƒ ì„¤ì •",
        "ë²ˆì—­ ì„¤ì •",
        "ë²ˆì—­ beta",
        "Translated by",
        "ì „ì²´ ë§¥ë½ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ë³¸ë¬¸ ë³´ê¸°ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.",
        "ìš”ì•½ë¬¸ì´ë¯€ë¡œ ì¼ë¶€ ë‚´ìš©ì´ ìƒëµë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "ìš”ì•½ë³¸ì´ ìë™ìš”ì•½ ê¸°ì‚¬ ì œëª©ê³¼ ì£¼ìš” ë¬¸ì¥ì„ ê¸°ë°˜ìœ¼ë¡œ ìë™ìš”ì•½í•œ ê²°ê³¼ì…ë‹ˆë‹¤",
        "ê¸°ì‚¬ ì œëª©ê³¼ ì£¼ìš” ë¬¸ì¥ì„ ê¸°ë°˜ìœ¼ë¡œ ìë™ìš”ì•½í•œ ê²°ê³¼ì…ë‹ˆë‹¤",
        # ì–¸ì–´ ëª©ë¡ í‚¤ì›Œë“œ
        "í•œêµ­ì–´ - English",
        "í•œêµ­ì–´ - ì˜ì–´",
        "English",
        "æ—¥æœ¬èª",
        "ç®€ä½“ä¸­æ–‡",
        "Deutsch",
        "Ğ ÑƒÑÑĞºĞ¸Ğ¹",
        "EspaÃ±ol",
        "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
        "bahasa Indonesia",
        "à¸ à¸²à¸©à¸²à¹„à¸—à¸¢",
        "TÃ¼rkÃ§e",
    ]

    clean_lines = []
    for l in lines:
        # 1) ê³µí†µ ë¸”ë™ë¦¬ìŠ¤íŠ¸
        if any(b in l for b in blacklist):
            continue

        # 2) ì‚¬ì§„/ê¸°ì‚¬ í¬ë ˆë”§ í•œ ì¤„ í†µì§¸ë¡œ ë‚ ë¦¬ê¸°
        if re.match(r"^\[[^]]{2,60}\]\s*[^ ]{1,20}\s*(ê¸°ì|í†µì‹ ì›|íŠ¹íŒŒì›)?\s*$", l):
            continue

        clean_lines.append(l)

    text = " ".join(clean_lines)

    # 3ë‹¨ê³„: ë³¸ë¬¸ ì•ˆì— ë¼ì–´ ìˆëŠ” í¬ë ˆë”§ íŒ¨í„´ ì œê±°
    text = re.sub(
        r"\[[^]]{2,60}(ì¼ë³´|ë‰´ìŠ¤|ì½”ë¦¬ì•„|KOREA|í¬í¬íˆ¬|ë² ìŠ¤íŠ¸ ì¼ë ˆë¸)[^]]*?\]\s*[^ ]{1,20}\s*(ê¸°ì|í†µì‹ ì›|íŠ¹íŒŒì›)?",
        "",
        text,
    )
    text = re.sub(
        r"\[[^]]{2,60}\]\s*[^ ]{1,20}\s*(ê¸°ì|í†µì‹ ì›|íŠ¹íŒŒì›)",
        "",
        text,
    )

    # 4ë‹¨ê³„: "ìš”ì•½ë³´ê¸° ìë™ìš”ì•½" ê¼¬ë¦¬ ì œê±°
    text = re.sub(r"ìš”ì•½ë³´ê¸°\s*ìë™ìš”ì•½.*$", "", text)

    # 5ë‹¨ê³„: ê³µë°± ì •ë¦¬
    text = re.sub(r"\s{2,}", " ", text).strip()

    return text


def remove_title_prefix(title: str, body: str) -> str:
    """
    ë³¸ë¬¸ì´ ì œëª©ìœ¼ë¡œ ì‹œì‘í•˜ë©´ ê·¸ ë¶€ë¶„ì„ ì˜ë¼ë‚¸ë‹¤.
    (ì œëª©ì´ ê·¸ëŒ€ë¡œ summary ì— ë°˜ë³µë˜ëŠ” í˜„ìƒ ì™„í™”ìš©)
    """
    if not title or not body:
        return body

    t = title.strip().strip('\"â€œâ€')
    b = body.strip()

    candidates = [
        t,
        f'"{t}"',
        f"â€œ{t}â€",
    ]

    for cand in candidates:
        if b.startswith(cand):
            return b[len(cand):].lstrip(" -â€“:Â·,\"'")

    return b

def parse_maz_overseas_row(tr) -> dict | None:
    """
    mazgtv í•´ì™¸ë¶„ì„ í…Œì´ë¸”ì˜ <tr> í•˜ë‚˜ì—ì„œ
    ë¦¬ê·¸ëª… / í™ˆíŒ€ / ì›ì •íŒ€ / í‚¥ì˜¤í”„ ì‹œê°„ / ìƒì„¸ ë§í¬ë¥¼ ì¶”ì¶œí•œë‹¤.
    """
    tds = tr.find_all("td")
    if len(tds) < 3:
        return None

    # í™ˆíŒ€
    home_parts = list(tds[0].stripped_strings)
    home_team = home_parts[0] if home_parts else ""

    # ê°€ìš´ë°: [ë¦¬ê·¸, VS, ë‚ ì§œ/ì‹œê°„] êµ¬ì¡°ë¼ê³  ê°€ì •
    center_parts = list(tds[1].stripped_strings)
    league = center_parts[0] if center_parts else ""
    kickoff = center_parts[-1] if center_parts else ""

    # ì›ì •íŒ€
    away_parts = list(tds[2].stripped_strings)
    away_team = away_parts[0] if away_parts else ""

    # ìƒì„¸ ë§í¬ (tr ì•ˆì— ìˆëŠ” ì²« ë²ˆì§¸ <a href>)
    a = tr.select_one("a[href]") or tr.find("a", href=True)
    url = a["href"].strip() if a and a.get("href") else ""

    return {
        "league": league.strip(),
        "home": home_team.strip(),
        "away": away_team.strip(),
        "kickoff": kickoff.strip(),
        "url": url,
    }

def _load_analysis_sheet(sh, sheet_name: str) -> dict:
    """
    êµ¬ê¸€ì‹œíŠ¸ì—ì„œ í•œ íƒ­(today / tomorrow)ì„ ì½ì–´ì„œ
    { sport: [ {id,title,summary}, ... ] } êµ¬ì¡°ë¡œ ë³€í™˜
    """
    try:
        ws = sh.worksheet(sheet_name)
    except Exception as e:
        print(f"[GSHEET] ì‹œíŠ¸ '{sheet_name}' ì—´ê¸° ì‹¤íŒ¨: {e}")
        return {}

    rows = ws.get_all_values()
    if not rows:
        return {}

    header = rows[0]
    idx_sport = 0
    idx_id = 1
    idx_title = 2
    idx_summary = 3

    def safe_index(name, default):
        try:
            return header.index(name)
        except ValueError:
            return default

    idx_sport = safe_index("sport", idx_sport)
    idx_id = safe_index("id", idx_id)
    idx_title = safe_index("title", idx_title)
    idx_summary = safe_index("summary", idx_summary)

    data: dict[str, list[dict]] = {}

    for row in rows[1:]:
        if len(row) <= idx_title:
            continue

        sport = (row[idx_sport] if len(row) > idx_sport else "").strip()
        if not sport:
            continue

        item_id = (row[idx_id] if len(row) > idx_id else "").strip()
        title = (row[idx_title] if len(row) > idx_title else "").strip()
        summary = (row[idx_summary] if len(row) > idx_summary else "").strip()

        if not title:
            continue

        if not item_id:
            cur_len = len(data.get(sport, []))
            item_id = f"{sport}_{cur_len + 1}"

        entry = {
            "id": item_id,
            "title": title,
            "summary": summary,
        }
        data.setdefault(sport, []).append(entry)

    return data


def reload_analysis_from_sheet():
    """
    êµ¬ê¸€ì‹œíŠ¸ì—ì„œ today / tomorrow íƒ­ì„ ì½ì–´ì„œ
    ANALYSIS_TODAY / ANALYSIS_TOMORROW / ANALYSIS_DATA_MAP ê°±ì‹ 
    """
    global ANALYSIS_TODAY, ANALYSIS_TOMORROW, ANALYSIS_DATA_MAP

    client = get_gs_client()
    spreadsheet_id = os.getenv("SPREADSHEET_ID")

    if not client or not spreadsheet_id:
        print("[GSHEET] ì‹œíŠ¸ í´ë¼ì´ì–¸íŠ¸ ë˜ëŠ” SPREADSHEET_ID ì—†ìŒ â†’ ê¸°ì¡´ í•˜ë“œì½”ë”© ë°ì´í„° ì‚¬ìš©")
        return

    try:
        sh = client.open_by_key(spreadsheet_id)
    except Exception as e:
        print(f"[GSHEET] ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—´ê¸° ì‹¤íŒ¨: {e}")
        return

    sheet_today_name = os.getenv("SHEET_TODAY_NAME", "today")
    sheet_tomorrow_name = os.getenv("SHEET_TOMORROW_NAME", "tomorrow")

    print(f"[GSHEET] '{sheet_today_name}' / '{sheet_tomorrow_name}' íƒ­ì—ì„œ ë¶„ì„ ë°ì´í„° ë¡œë”© ì‹œë„")

    try:
        today_data = _load_analysis_sheet(sh, sheet_today_name)
        tomorrow_data = _load_analysis_sheet(sh, sheet_tomorrow_name)
    except Exception as e:
        print(f"[GSHEET] ì‹œíŠ¸ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        return

    ANALYSIS_TODAY = today_data
    ANALYSIS_TOMORROW = tomorrow_data

    ANALYSIS_DATA_MAP = {
        "today": ANALYSIS_TODAY,
        "tomorrow": ANALYSIS_TOMORROW,
    }

    print("[GSHEET] ANALYSIS_TODAY / ANALYSIS_TOMORROW ê°±ì‹  ì™„ë£Œ")

def append_analysis_rows(day_key: str, rows: list[list[str]]) -> bool:
    """
    ë¶„ì„ ë°ì´í„°ë¥¼ today / tomorrow íƒ­ì— ì¶”ê°€í•˜ëŠ” ê³µìš© í•¨ìˆ˜.
    rows: [ [sport, "", title, summary], ... ]
    """
    client_gs = get_gs_client()
    spreadsheet_id = os.getenv("SPREADSHEET_ID")

    if not (client_gs and spreadsheet_id):
        print("[GSHEET][ANALYSIS] ì„¤ì • ì—†ìŒ â†’ ì €ì¥ ë¶ˆê°€")
        return False

    sheet_today_name = os.getenv("SHEET_TODAY_NAME", "today")
    sheet_tomorrow_name = os.getenv("SHEET_TOMORROW_NAME", "tomorrow")
    sheet_name = sheet_today_name if day_key == "today" else sheet_tomorrow_name

    try:
        sh = client_gs.open_by_key(spreadsheet_id)
        ws = sh.worksheet(sheet_name)
    except Exception as e:
        print(f"[GSHEET][ANALYSIS] ì‹œíŠ¸ '{sheet_name}' ì—´ê¸° ì‹¤íŒ¨: {e}")
        return False

    try:
        ws.append_rows(rows, value_input_option="RAW")
        print(f"[GSHEET][ANALYSIS] {sheet_name} ì— {len(rows)}ê±´ ì¶”ê°€")
        return True
    except Exception as e:
        print(f"[GSHEET][ANALYSIS] append_rows ì˜¤ë¥˜: {e}")
        return False

def _get_ws_by_name(sh, name: str):
    try:
        return sh.worksheet(name)
    except Exception:
        return None

def get_site_export_ws():
    """
    site_export íƒ­ ì›Œí¬ì‹œíŠ¸ ë°˜í™˜.
    ì—†ìœ¼ë©´ ìƒì„± ì‹œë„(ê¶Œí•œ/í™˜ê²½ì— ë”°ë¼ ì‹¤íŒ¨ ê°€ëŠ¥).
    """
    client_gs = get_gs_client()
    spreadsheet_id = os.getenv("SPREADSHEET_ID")
    if not (client_gs and spreadsheet_id):
        return None

    sheet_name = os.getenv("SHEET_SITE_EXPORT_NAME", "site_export")

    try:
        sh = client_gs.open_by_key(spreadsheet_id)
        ws = _get_ws_by_name(sh, sheet_name)
        if ws:
            return ws

        # ì—†ìœ¼ë©´ ìƒì„± ì‹œë„
        ws = sh.add_worksheet(title=sheet_name, rows=2000, cols=10)
        # í—¤ë” ì„¸íŒ…
        ws.update("A1", [[
            "day", "sport", "src_id", "title", "body", "creatadAt"
        ]])
        return ws

    except Exception as e:
        print(f"[GSHEET][SITE_EXPORT] ì›Œí¬ì‹œíŠ¸ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
        return None


def get_existing_site_src_ids(day_value: str | None = None) -> set[str]:
    """
    site_export íƒ­ì—ì„œ ì´ë¯¸ ì €ì¥ëœ src_id ëª©ë¡ì„ ì½ì–´ ì¤‘ë³µ ì €ì¥ ë°©ì§€.
    day_valueë¥¼ ì£¼ë©´ í•´ë‹¹ dayë§Œ í•„í„°ë§í•´ì„œ ì½ëŠ”ë‹¤.
    """
    ws = get_site_export_ws()
    if not ws:
        return set()

    try:
        values = ws.get_all_values()
        if not values or len(values) < 2:
            return set()

        header = values[0]
        idx_day = header.index("day") if "day" in header else 0
        idx_src = header.index("src_id") if "src_id" in header else 2

        out = set()
        for r in values[1:]:
            if len(r) <= idx_src:
                continue
            rid = (r[idx_src] or "").strip()
            if not rid:
                continue
            if day_value:
                dv = (r[idx_day] or "").strip() if len(r) > idx_day else ""
                if dv != day_value:
                    continue
            out.add(rid)

        return out

    except Exception as e:
        print(f"[GSHEET][SITE_EXPORT] existing src_id ë¡œë“œ ì‹¤íŒ¨: {e}")
        return set()


def append_site_export_rows(rows: list[list[str]]) -> bool:
    """
    site_export íƒ­ì— rowsë¥¼ appendí•œë‹¤.
    rows í¬ë§·: [day, sport, src_id, title, body, creatadAt]
    """
    ws = get_site_export_ws()
    if not ws:
        print("[GSHEET][SITE_EXPORT] ì›Œí¬ì‹œíŠ¸ ì—†ìŒ â†’ ì €ì¥ ë¶ˆê°€")
        return False

    try:
        ws.append_rows(rows, value_input_option="RAW")
        print(f"[GSHEET][SITE_EXPORT] {len(rows)}ê±´ ì¶”ê°€")
        return True
    except Exception as e:
        print(f"[GSHEET][SITE_EXPORT] append_rows ì‹¤íŒ¨: {e}")
        return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ site_export ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SITE_EXPORT_SHEET_NAME = os.getenv("SHEET_SITE_EXPORT_NAME", "site_export")
SITE_EXPORT_HEADER = ["day", "sport", "src_id", "title", "body", "creatadAt"]  # í—¤ë” ì˜¤íƒ€ í¬í•¨ ê·¸ëŒ€ë¡œ

def _ensure_header(ws, header: list[str]) -> None:
    """ì‹œíŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ í—¤ë”ê°€ ì—†ìœ¼ë©´ í—¤ë”ë¥¼ 1í–‰ì— ê¹”ì•„ì¤€ë‹¤."""
    try:
        values = ws.get_all_values()
        if not values:
            ws.update("A1", [header])
            return
        first = values[0]
        if [c.strip() for c in first] != header:
            # í—¤ë”ê°€ ë‹¤ë¥´ë©´ ê°•ì œë¡œ êµì²´í•˜ì§„ ì•Šê³ , ì—†ëŠ” ê²½ìš°ë§Œ ê¹”ê¸°
            # (ì›í•˜ë©´ ì—¬ê¸°ì„œ ê°•ì œ êµì²´ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŒ)
            pass
    except Exception as e:
        print(f"[GSHEET][SITE_EXPORT] í—¤ë” í™•ì¸ ì‹¤íŒ¨: {e}")

def append_site_export_rows(rows: list[list[str]]) -> bool:
    """
    site_export íƒ­ì— rowsë¥¼ append.
    rows: [ [day, sport, src_id, title, body, createdAt], ... ]
    """
    client_gs = get_gs_client()
    spreadsheet_id = os.getenv("SPREADSHEET_ID")
    if not (client_gs and spreadsheet_id):
        print("[GSHEET][SITE_EXPORT] ì„¤ì • ì—†ìŒ â†’ ì €ì¥ ë¶ˆê°€")
        return False

    try:
        sh = client_gs.open_by_key(spreadsheet_id)
        ws = sh.worksheet(SITE_EXPORT_SHEET_NAME)
        _ensure_header(ws, SITE_EXPORT_HEADER)
    except Exception as e:
        print(f"[GSHEET][SITE_EXPORT] ì‹œíŠ¸ '{SITE_EXPORT_SHEET_NAME}' ì—´ê¸° ì‹¤íŒ¨: {e}")
        return False

    try:
        ws.append_rows(rows, value_input_option="RAW")
        print(f"[GSHEET][SITE_EXPORT] {SITE_EXPORT_SHEET_NAME} ì— {len(rows)}ê±´ ì¶”ê°€")
        return True
    except Exception as e:
        print(f"[GSHEET][SITE_EXPORT] append_rows ì˜¤ë¥˜: {e}")
        return False

def get_existing_site_src_ids(day_str: str) -> set[str]:
    """site_export íƒ­ì—ì„œ dayê°€ ê°™ì€ í–‰ë“¤ì˜ src_idë¥¼ setìœ¼ë¡œ ê°€ì ¸ì™€ ì¤‘ë³µ ì €ì¥ ë°©ì§€."""
    client_gs = get_gs_client()
    spreadsheet_id = os.getenv("SPREADSHEET_ID")
    if not (client_gs and spreadsheet_id):
        return set()

    try:
        sh = client_gs.open_by_key(spreadsheet_id)
        ws = sh.worksheet(SITE_EXPORT_SHEET_NAME)
        values = ws.get_all_values()
        if not values or len(values) < 2:
            return set()

        header = values[0]
        idx_day = header.index("day") if "day" in header else 0
        idx_src = header.index("src_id") if "src_id" in header else 2

        out = set()
        for r in values[1:]:
            if len(r) <= max(idx_day, idx_src):
                continue
            if (r[idx_day] or "").strip() == day_str:
                sid = (r[idx_src] or "").strip()
                if sid:
                    out.add(sid)
        return out
    except Exception as e:
        print(f"[GSHEET][SITE_EXPORT] ê¸°ì¡´ src_id ë¡œë”© ì‹¤íŒ¨: {e}")
        return set()

def get_existing_analysis_ids(day_key: str) -> set[str]:
    """
    today / tomorrow ì‹œíŠ¸ì—ì„œ ì´ë¯¸ ì €ì¥ëœ id ê°’ë“¤ì„ setìœ¼ë¡œ ê°€ì ¸ì˜¨ë‹¤.
    (ì¤‘ë³µ í¬ë¡¤ë§ ë°©ì§€ìš©)
    """
    client_gs = get_gs_client()
    spreadsheet_id = os.getenv("SPREADSHEET_ID")

    if not (client_gs and spreadsheet_id):
        return set()

    sheet_today_name = os.getenv("SHEET_TODAY_NAME", "today")
    sheet_tomorrow_name = os.getenv("SHEET_TOMORROW_NAME", "tomorrow")
    sheet_name = sheet_today_name if day_key == "today" else sheet_tomorrow_name

    try:
        sh = client_gs.open_by_key(spreadsheet_id)
        ws = sh.worksheet(sheet_name)
    except Exception:
        return set()

    rows = ws.get_all_values()
    if not rows:
        return set()

    header = rows[0]

    def safe_index(name, default):
        try:
            return header.index(name)
        except ValueError:
            return default

    idx_sport = safe_index("sport", 0)
    idx_id = safe_index("id", 1)

    existing: set[str] = set()
    for row in rows[1:]:
        if len(row) <= idx_id:
            continue
        row_id = (row[idx_id] if len(row) > idx_id else "").strip()
        if row_id:
            existing.add(row_id)

    return existing

NEWS_DATA = {}


def _load_news_sheet(sh, sheet_name: str) -> dict:
    """
    êµ¬ê¸€ì‹œíŠ¸ì—ì„œ ë‰´ìŠ¤ íƒ­ì„ ì½ì–´ì„œ
    {
        sport: [ {id,title,summary}, ... ]
    } êµ¬ì¡°ë¡œ ë³€í™˜
    """
    try:
        ws = sh.worksheet(sheet_name)
    except Exception as e:
        print(f"[GSHEET] ë‰´ìŠ¤ ì‹œíŠ¸ '{sheet_name}' ì—´ê¸° ì‹¤íŒ¨: {e}")
        return {}

    rows = ws.get_all_values()
    if not rows:
        return {}

    header = rows[0]

    idx_sport = 0
    idx_id = 1
    idx_title = 2
    idx_summary = 3

    def safe_index(name, default):
        try:
            return header.index(name)
        except ValueError:
            return default

    idx_sport = safe_index("sport", idx_sport)
    idx_id = safe_index("id", idx_id)
    idx_title = safe_index("title", idx_title)
    idx_summary = safe_index("summary", idx_summary)

    data: dict[str, list[dict]] = {}

    for row in rows[1:]:
        if len(row) <= idx_title:
            continue

        sport = (row[idx_sport] if len(row) > idx_sport else "").strip()
        if not sport:
            continue

        item_id = (row[idx_id] if len(row) > idx_id else "").strip()
        title = (row[idx_title] if len(row) > idx_title else "").strip()
        summary = (row[idx_summary] if len(row) > idx_summary else "").strip()

        if not title:
            continue

        if not item_id:
            cur_len = len(data.get(sport, []))
            item_id = f"{sport}_news_{cur_len + 1}"

        entry = {
            "id": item_id,
            "title": title,
            "summary": summary,
        }
        data.setdefault(sport, []).append(entry)

    return data


def reload_news_from_sheet():
    """êµ¬ê¸€ì‹œíŠ¸ì—ì„œ ë‰´ìŠ¤ íƒ­ì„ ì½ì–´ì„œ NEWS_DATA ê°±ì‹ """
    global NEWS_DATA
    client = get_gs_client()
    spreadsheet_id = os.getenv("SPREADSHEET_ID")

    if not client or not spreadsheet_id:
        print("[GSHEET] ë‰´ìŠ¤ìš© SPREADSHEET ì—°ë™ ì‹¤íŒ¨ â†’ ê¸°ì¡´ í•˜ë“œì½”ë”© NEWS_DATA ì‚¬ìš©.")
        return

    try:
        sh = client.open_by_key(spreadsheet_id)
    except Exception as e:
        print(f"[GSHEET] ë‰´ìŠ¤ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—´ê¸° ì‹¤íŒ¨: {e}")
        return

    sheet_news_name = os.getenv("SHEET_NEWS_NAME", "news")
    print(f"[GSHEET] '{sheet_news_name}' íƒ­ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë¡œë”© ì‹œë„")

    try:
        news_data = _load_news_sheet(sh, sheet_news_name)
    except Exception as e:
        print(f"[GSHEET] ë‰´ìŠ¤ ì‹œíŠ¸ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        return

    NEWS_DATA = news_data
    print("[GSHEET] NEWS_DATA ê°±ì‹  ì™„ë£Œ")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í‚¤ë³´ë“œ/ë©”ë‰´ êµ¬ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_reply_keyboard() -> ReplyKeyboardMarkup:
    """ë´‡ 1:1 í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨ í•˜ë‹¨ í‚¤ë³´ë“œ"""
    menu = [
        ["ë©”ë‰´ ë¯¸ë¦¬ë³´ê¸°", "ë„ì›€ë§"],
    ]
    return ReplyKeyboardMarkup(menu, resize_keyboard=True)


def build_main_inline_menu() -> InlineKeyboardMarkup:
    """
    ë©”ì¸ ì¸ë¼ì¸ ë©”ë‰´ (ì±„ë„/ë¯¸ë¦¬ë³´ê¸° ê³µí†µ)
    ì±„ë„ì—ì„œëŠ” ì´ ë²„íŠ¼ì„ ëˆŒëŸ¬ ê°ì ë´‡ DMìœ¼ë¡œ ì´ë™í•˜ê²Œ í•¨.
    """
    today_str, tomorrow_str = get_date_labels()

    buttons = [
        [InlineKeyboardButton("ì‹¤ì‹œê°„ ë¬´ë£Œ ì¤‘ê³„", url="https://goat-tv.com")],
        [
            InlineKeyboardButton(
                f"{today_str} ê²½ê¸° ë¶„ì„í”½",
                url=f"https://t.me/{BOT_USERNAME}?start=today",
            )
        ],
        [
            InlineKeyboardButton(
                f"{tomorrow_str} ê²½ê¸° ë¶„ì„í”½",
                url=f"https://t.me/{BOT_USERNAME}?start=tomorrow",
            )
        ],
        [
            InlineKeyboardButton(
                "ìŠ¤í¬ì¸  ë‰´ìŠ¤ ìš”ì•½",
                url=f"https://t.me/{BOT_USERNAME}?start=news",
            )
        ],
    ]
    return InlineKeyboardMarkup(buttons)


def build_analysis_category_menu(key: str) -> InlineKeyboardMarkup:
    # key = "today" or "tomorrow"
    buttons = [
        [InlineKeyboardButton("âš½ï¸ì¶•êµ¬âš½ï¸", callback_data=f"analysis_cat:{key}:ì¶•êµ¬")],
        [InlineKeyboardButton("ğŸ€ë†êµ¬ğŸ€", callback_data=f"analysis_cat:{key}:ë†êµ¬")],
        [InlineKeyboardButton("âš¾ï¸ì•¼êµ¬âš¾ï¸", callback_data=f"analysis_cat:{key}:ì•¼êµ¬")],
        [InlineKeyboardButton("ğŸë°°êµ¬ğŸ", callback_data=f"analysis_cat:{key}:ë°°êµ¬")],
        [InlineKeyboardButton("â—€ ë©”ì¸ ë©”ë‰´ë¡œ", callback_data="back_main")],
    ]
    return InlineKeyboardMarkup(buttons)

def build_soccer_subcategory_menu(key: str) -> InlineKeyboardMarkup:
    """
    ì¶•êµ¬ ì„ íƒ í›„ ë‚˜ì˜¤ëŠ” 2ë‹¨ê³„ ë©”ë‰´:
    í•´ì™¸ì¶•êµ¬ / Kë¦¬ê·¸ / Jë¦¬ê·¸
    key = "today" ë˜ëŠ” "tomorrow"
    """
    buttons = [
        [InlineKeyboardButton("í•´ì™¸ì¶•êµ¬", callback_data=f"soccer_cat:{key}:í•´ì™¸ì¶•êµ¬")],
        [InlineKeyboardButton("Kë¦¬ê·¸", callback_data=f"soccer_cat:{key}:Kë¦¬ê·¸")],
        [InlineKeyboardButton("Jë¦¬ê·¸", callback_data=f"soccer_cat:{key}:Jë¦¬ê·¸")],
        [InlineKeyboardButton("â—€ ì¢…ëª© ì„ íƒìœ¼ë¡œ", callback_data=f"analysis_root:{key}")],
        [InlineKeyboardButton("â—€ ë©”ì¸ ë©”ë‰´ë¡œ", callback_data="back_main")],
    ]
    return InlineKeyboardMarkup(buttons)

def build_basketball_subcategory_menu(key: str) -> InlineKeyboardMarkup:
    """
    ë†êµ¬ ì„ íƒ í›„ ë‚˜ì˜¤ëŠ” 2ë‹¨ê³„ ë©”ë‰´:
    NBA / KBL
    key = "today" ë˜ëŠ” "tomorrow"
    """
    buttons = [
        [InlineKeyboardButton("NBA", callback_data=f"basket_cat:{key}:NBA")],
        [InlineKeyboardButton("KBL", callback_data=f"basket_cat:{key}:KBL")],
        [InlineKeyboardButton("â—€ ì¢…ëª© ì„ íƒìœ¼ë¡œ", callback_data=f"analysis_root:{key}")],
        [InlineKeyboardButton("â—€ ë©”ì¸ ë©”ë‰´ë¡œ", callback_data="back_main")],
    ]
    return InlineKeyboardMarkup(buttons)

def build_baseball_subcategory_menu(key: str) -> InlineKeyboardMarkup:
    """
    ì•¼êµ¬ ì„ íƒ ì‹œ ë‚˜ì˜¤ëŠ” í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ë©”ë‰´:
    - í•´ì™¸ì•¼êµ¬
    - KBO
    - NPB
    """
    buttons = [
        [InlineKeyboardButton("âš¾ í•´ì™¸ì•¼êµ¬", callback_data=f"baseball_cat:{key}:í•´ì™¸ì•¼êµ¬")],
        [InlineKeyboardButton("âš¾ KBO", callback_data=f"baseball_cat:{key}:KBO")],
        [InlineKeyboardButton("âš¾ NPB", callback_data=f"baseball_cat:{key}:NPB")],
        [InlineKeyboardButton("â—€ ì¢…ëª© ì„ íƒìœ¼ë¡œ", callback_data=f"analysis_root:{key}")],
        [InlineKeyboardButton("â—€ ë©”ì¸ ë©”ë‰´ë¡œ", callback_data="back_main")],
    ]
    return InlineKeyboardMarkup(buttons)

def build_volleyball_subcategory_menu(key: str) -> InlineKeyboardMarkup:
    """
    ë°°êµ¬ ì„ íƒ ì‹œ ë‚˜ì˜¤ëŠ” í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ë©”ë‰´
    (í˜„ì¬ëŠ” Vë¦¬ê·¸ë§Œ ìˆì§€ë§Œ, ë‚˜ì¤‘ì— í•´ì™¸ë°°êµ¬ ë“±ì„ ëŠ˜ë¦´ ìˆ˜ ìˆìŒ)
    """
    buttons = [
        [InlineKeyboardButton("Vë¦¬ê·¸", callback_data=f"volley_cat:{key}:Vë¦¬ê·¸")],
        [InlineKeyboardButton("â—€ ì¢…ëª© ì„ íƒìœ¼ë¡œ", callback_data=f"analysis_root:{key}")],
        [InlineKeyboardButton("â—€ ë©”ì¸ ë©”ë‰´ë¡œ", callback_data="back_main")],
    ]
    return InlineKeyboardMarkup(buttons)


def build_analysis_match_menu(key: str, sport: str, page: int = 1) -> InlineKeyboardMarkup:
    """ì¢…ëª© ì„ íƒ í›„ â†’ í•´ë‹¹ ì¢…ëª© ê²½ê¸° ë¦¬ìŠ¤íŠ¸ ë©”ë‰´ (10ê°œì”© í˜ì´ì§€ ë‚˜ëˆ„ê¸°)"""
    items = ANALYSIS_DATA_MAP.get(key, {}).get(sport, [])
    per_page = 10

    if page < 1:
        page = 1

    total = len(items)
    total_pages = max(1, math.ceil(total / per_page))

    if page > total_pages:
        page = total_pages

    start = (page - 1) * per_page
    end = start + per_page
    page_items = items[start:end]

    buttons: list[list[InlineKeyboardButton]] = []

    # í˜„ì¬ í˜ì´ì§€ì˜ ê²½ê¸°ë“¤ë§Œ ë²„íŠ¼ìœ¼ë¡œ
    for item in page_items:
        cb = f"match:{key}:{sport}:{item['id']}"
        buttons.append([InlineKeyboardButton(item["title"], callback_data=cb)])

    # í˜ì´ì§€ ì´ë™ ë²„íŠ¼ (ì´ì „ / í˜„ì¬í˜ì´ì§€ / ë‹¤ìŒ)
    if total_pages > 1:
        nav_row: list[InlineKeyboardButton] = []

        if page > 1:
            nav_row.append(
                InlineKeyboardButton(
                    "â—€ ì´ì „",
                    callback_data=f"match_page:{key}:{sport}:{page-1}",
                )
            )

        nav_row.append(
            InlineKeyboardButton(
                f"{page}/{total_pages}",
                callback_data="noop",  # ëˆŒëŸ¬ë„ ì•„ë¬´ ë™ì‘ ì•ˆ í•˜ëŠ” ìš©ë„
            )
        )

        if page < total_pages:
            nav_row.append(
                InlineKeyboardButton(
                    "ë‹¤ìŒ â–¶",
                    callback_data=f"match_page:{key}:{sport}:{page+1}",
                )
            )

        buttons.append(nav_row)

    # ê³µí†µ í•˜ë‹¨ ë²„íŠ¼
    buttons.append(
        [InlineKeyboardButton("â—€ ì¢…ëª© ì„ íƒìœ¼ë¡œ", callback_data=f"analysis_root:{key}")]
    )
    buttons.append([InlineKeyboardButton("â—€ ë©”ì¸ ë©”ë‰´ë¡œ", callback_data="back_main")])

    return InlineKeyboardMarkup(buttons)

def build_news_category_menu() -> InlineKeyboardMarkup:
    """ìŠ¤í¬ì¸  ë‰´ìŠ¤ ìš”ì•½ â†’ ì¢…ëª© ì„ íƒ ë©”ë‰´"""
    buttons = [
        [InlineKeyboardButton("âš½ï¸ì¶•êµ¬ ë‰´ìŠ¤âš½ï¸", callback_data="news_cat:ì¶•êµ¬")],
        [InlineKeyboardButton("ğŸ€ë†êµ¬ ë‰´ìŠ¤ğŸ€", callback_data="news_cat:ë†êµ¬")],
        [InlineKeyboardButton("âš¾ï¸ì•¼êµ¬ ë‰´ìŠ¤âš¾ï¸", callback_data="news_cat:ì•¼êµ¬")],
        [InlineKeyboardButton("ğŸë°°êµ¬ ë‰´ìŠ¤ğŸ", callback_data="news_cat:ë°°êµ¬")],
        [InlineKeyboardButton("ê¸°íƒ€ì¢…ëª© ë‰´ìŠ¤", callback_data="news_cat:ê¸°íƒ€ì¢…")],
        [InlineKeyboardButton("â—€ ë©”ì¸ ë©”ë‰´ë¡œ", callback_data="back_main")],
    ]
    return InlineKeyboardMarkup(buttons)


def build_news_list_menu(sport: str) -> InlineKeyboardMarkup:
    """ì¢…ëª© ì„ íƒ í›„ â†’ í•´ë‹¹ ì¢…ëª© ë‰´ìŠ¤ ì œëª© ë¦¬ìŠ¤íŠ¸ ë©”ë‰´"""
    items = NEWS_DATA.get(sport, [])
    buttons = []
    for item in items:
        cb = f"news_item:{sport}:{item['id']}"
        buttons.append([InlineKeyboardButton(item["title"], callback_data=cb)])

    buttons.append([InlineKeyboardButton("â—€ ì¢…ëª© ì„ íƒìœ¼ë¡œ", callback_data="news_root")])
    buttons.append([InlineKeyboardButton("â—€ ë©”ì¸ ë©”ë‰´ë¡œ", callback_data="back_main")])
    return InlineKeyboardMarkup(buttons)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê³µí†µ: ë©”ì¸ ë©”ë‰´ ë³´ë‚´ëŠ” í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def send_main_menu(chat_id: int | str, context: ContextTypes.DEFAULT_TYPE, preview: bool = False):
    """
    ì±„ë„/DM ê³µí†µìœ¼ë¡œ 'í…ìŠ¤íŠ¸ + ë©”ì¸ ë©”ë‰´ ë²„íŠ¼' ì „ì†¡.
    """
    msg = await context.bot.send_message(
        chat_id=chat_id,
        text=get_menu_caption(),
        reply_markup=build_main_inline_menu(),
    )
    return msg


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•¸ë“¤ëŸ¬ë“¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# 1) /start â€“ DMì—ì„œ ì±„ë„ê³¼ ë™ì¼í•œ ë ˆì´ì•„ì›ƒ or ë°”ë¡œ ë©”ë‰´ ì§„ì…
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    chat_id = update.effective_chat.id
    args = context.args
    mode = args[0] if args else None

    today_str, tomorrow_str = get_date_labels()

    if mode == "today":
        await update.message.reply_text(
            f"{today_str} ê²½ê¸° ë¶„ì„í”½ ë©”ë‰´ì…ë‹ˆë‹¤. ì¢…ëª©ì„ ì„ íƒí•˜ì„¸ìš” ğŸ‘‡",
            reply_markup=build_analysis_category_menu("today"),
        )
        return

    if mode == "tomorrow":
        await update.message.reply_text(
            f"{tomorrow_str} ê²½ê¸° ë¶„ì„í”½ ë©”ë‰´ì…ë‹ˆë‹¤. ì¢…ëª©ì„ ì„ íƒí•˜ì„¸ìš” ğŸ‘‡",
            reply_markup=build_analysis_category_menu("tomorrow"),
        )
        return

    if mode == "news":
        await update.message.reply_text(
            "ìŠ¤í¬ì¸  ë‰´ìŠ¤ ìš”ì•½ì…ë‹ˆë‹¤. ì¢…ëª©ì„ ì„ íƒí•˜ì„¸ìš” ğŸ‘‡",
            reply_markup=build_news_category_menu(),
        )
        return

    await update.message.reply_text(
        "ìŠ¤í¬ì¸ ë´‡ì…ë‹ˆë‹¤.\n"
        "ì•„ë˜ì—ëŠ” ì±„ë„ì— ì˜¬ë¼ê°ˆ ë©”ë‰´ì™€ ë™ì¼í•œ ë ˆì´ì•„ì›ƒ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ë³´ì—¬ì¤„ê²Œ.\n"
        "ì‹¤ì œ ì±„ë„ ë°°í¬ëŠ” /publish ëª…ë ¹ìœ¼ë¡œ ì§„í–‰í•˜ë©´ ë¼.",
        reply_markup=build_reply_keyboard(),
    )

    await send_main_menu(chat_id, context, preview=True)


async def myid(update: Update, context: ContextTypes.DEFAULT_TYPE):
    uid = update.effective_user.id
    await update.message.reply_text(f"ë‹¹ì‹ ì˜ í…”ë ˆê·¸ë¨ ID: {uid}")


# 2) DM í…ìŠ¤íŠ¸ ì²˜ë¦¬ â€“ ê°„ë‹¨ í…ŒìŠ¤íŠ¸ìš©
async def on_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = (update.message.text or "").strip()
    if "ë©”ë‰´ ë¯¸ë¦¬ë³´ê¸°" in text:
        await start(update, context)
    elif "ë„ì›€ë§" in text:
        await update.message.reply_text(
            "/start : ë©”ë‰´ ë¯¸ë¦¬ë³´ê¸°\n"
            "/publish : ì±„ë„ì— ë©”ë‰´ ì „ì†¡ + ìƒë‹¨ ê³ ì •"
        )
    else:
        await update.message.reply_text("ë©”ë‰´ ë¯¸ë¦¬ë³´ê¸°ëŠ” /start ë˜ëŠ” 'ë©”ë‰´ ë¯¸ë¦¬ë³´ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")


# 3) /publish â€“ ì±„ë„ë¡œ ë©”ë‰´ ë³´ë‚´ê³  ìƒë‹¨ ê³ ì •
async def publish(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not is_admin(update):
        await update.message.reply_text("ì´ ëª…ë ¹ì–´ëŠ” ê´€ë¦¬ìë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    if not CHANNEL_ID:
        await update.message.reply_text("CHANNEL_IDê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. Render í™˜ê²½ë³€ìˆ˜ì— CHANNEL_IDë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return

    try:
        await context.bot.unpin_all_chat_messages(CHANNEL_ID)
    except Exception:
        pass

    msg = await send_main_menu(CHANNEL_ID, context, preview=False)

    await context.bot.pin_chat_message(
        chat_id=CHANNEL_ID,
        message_id=msg.message_id,
        disable_notification=True,
    )

    await update.message.reply_text("ì±„ë„ì— ë©”ë‰´ë¥¼ ì˜¬ë¦¬ê³  ìƒë‹¨ì— ê³ ì •í–ˆìŠµë‹ˆë‹¤ âœ…")


# 5) /syncsheet â€“ êµ¬ê¸€ì‹œíŠ¸ì—ì„œ ë¶„ì„/ë‰´ìŠ¤ ë°ì´í„° ë‹¤ì‹œ ë¡œë”©
async def syncsheet(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not is_admin(update):
        await update.message.reply_text("ì´ ëª…ë ¹ì–´ëŠ” ê´€ë¦¬ìë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    try:
        reload_analysis_from_sheet()
        reload_news_from_sheet()
        await update.message.reply_text("êµ¬ê¸€ì‹œíŠ¸ì—ì„œ ë¶„ì„ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤ âœ…")
    except Exception as e:
        await update.message.reply_text(f"êµ¬ê¸€ì‹œíŠ¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


# ğŸ”¹ /newsclean â€“ news ì‹œíŠ¸ ì´ˆê¸°í™” (í—¤ë”ë§Œ ë‚¨ê¸°ê¸°)
async def newsclean(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not is_admin(update):
        await update.message.reply_text("ì´ ëª…ë ¹ì–´ëŠ” ê´€ë¦¬ìë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    client_gs = get_gs_client()
    spreadsheet_id = os.getenv("SPREADSHEET_ID")

    if not (client_gs and spreadsheet_id):
        await update.message.reply_text(
            "êµ¬ê¸€ì‹œíŠ¸ ì„¤ì •(GOOGLE_SERVICE_KEY ë˜ëŠ” SPREADSHEET_ID)ì´ ì—†ì–´ ì‹œíŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )
        return

    try:
        sh = client_gs.open_by_key(spreadsheet_id)
        ws = sh.worksheet(os.getenv("SHEET_NEWS_NAME", "news"))
    except Exception as e:
        await update.message.reply_text(f"ë‰´ìŠ¤ ì‹œíŠ¸ë¥¼ ì—´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
        return

    try:
        rows = ws.get_all_values()
        if rows:
            header = rows[0]
        else:
            header = ["sport", "id", "title", "summary"]

        ws.clear()
        ws.update("A1", [header])

        await update.message.reply_text("ë‰´ìŠ¤ ì‹œíŠ¸ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤. (í—¤ë”ë§Œ ë‚¨ê²¨ë‘ ) âœ…")

    except Exception as e:
        await update.message.reply_text(f"ì‹œíŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
        return

# ğŸ”¹ /allclean â€“ today / tomorrow / news ì‹œíŠ¸ ì „ì²´ ì´ˆê¸°í™”
async def allclean(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not is_admin(update):
        await update.message.reply_text("ì´ ëª…ë ¹ì–´ëŠ” ê´€ë¦¬ìë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    client_gs = get_gs_client()
    spreadsheet_id = os.getenv("SPREADSHEET_ID")

    if not (client_gs and spreadsheet_id):
        await update.message.reply_text(
            "êµ¬ê¸€ì‹œíŠ¸ ì„¤ì •(GOOGLE_SERVICE_KEY ë˜ëŠ” SPREADSHEET_ID)ì´ ì—†ì–´ ì‹œíŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )
        return

    try:
        sh = client_gs.open_by_key(spreadsheet_id)
    except Exception as e:
        await update.message.reply_text(f"ìŠ¤í”„ë ˆë“œì‹œíŠ¸ë¥¼ ì—´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
        return

    sheet_today_name = os.getenv("SHEET_TODAY_NAME", "today")
    sheet_tomorrow_name = os.getenv("SHEET_TOMORROW_NAME", "tomorrow")
    sheet_news_name = os.getenv("SHEET_NEWS_NAME", "news")

    sheet_configs = [
        (sheet_today_name, "today ë¶„ì„"),
        (sheet_tomorrow_name, "tomorrow ë¶„ì„"),
        (sheet_news_name, "news ë‰´ìŠ¤"),
    ]

    errors: list[str] = []

    for sheet_name, desc in sheet_configs:
        try:
            ws = sh.worksheet(sheet_name)
        except Exception as e:
            errors.append(f"{desc} ì‹œíŠ¸ë¥¼ ì—´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
            continue

        try:
            rows = ws.get_all_values()
            if rows:
                header = rows[0]
            else:
                # today / tomorrow / news ëª¨ë‘ ê°™ì€ í˜•ì‹ ì‚¬ìš©
                header = ["sport", "id", "title", "summary"]

            ws.clear()
            ws.update("A1", [header])
        except Exception as e:
            errors.append(f"{desc} ì‹œíŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")

    # ë©”ëª¨ë¦¬ ë°ì´í„°ë„ í•¨ê»˜ ë¦¬ì…‹
    reload_analysis_from_sheet()
    reload_news_from_sheet()

    if errors:
        msg = (
            "ì¼ë¶€ ì‹œíŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n\n"
            + "\n".join(errors)
        )
    else:
        msg = "today / tomorrow / news ì‹œíŠ¸ë¥¼ ëª¨ë‘ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤. (í—¤ë”ë§Œ ë‚¨ê²¨ë‘ ) âœ…"

    await update.message.reply_text(msg)

async def _analysis_clean_by_sports(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    *,
    sports_to_clear: set[str] | None,
    label: str,
):
    """
    tomorrow ì‹œíŠ¸ì—ì„œ sport ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ íŠ¹ì • ì¢…ëª©ë§Œ ì§€ìš°ê±°ë‚˜,
    sports_to_clear ê°€ None ì´ë©´ ì „ì²´(í—¤ë” ì œì™¸) ì‚­ì œ.
    """
    if not is_admin(update):
        await update.message.reply_text("ì´ ëª…ë ¹ì–´ëŠ” ê´€ë¦¬ìë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    client_gs = get_gs_client()
    spreadsheet_id = os.getenv("SPREADSHEET_ID")

    if not (client_gs and spreadsheet_id):
        await update.message.reply_text(
            "êµ¬ê¸€ì‹œíŠ¸ ì„¤ì •(GOOGLE_SERVICE_KEY ë˜ëŠ” SPREADSHEET_ID)ì´ ì—†ì–´ ì‹œíŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )
        return

    try:
        sh = client_gs.open_by_key(spreadsheet_id)
        ws = sh.worksheet(os.getenv("SHEET_TOMORROW_NAME", "tomorrow"))
    except Exception as e:
        await update.message.reply_text(f"tomorrow ì‹œíŠ¸ë¥¼ ì—´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
        return

    try:
        rows = ws.get_all_values()
    except Exception as e:
        await update.message.reply_text(f"ì‹œíŠ¸ ì½ê¸° ì˜¤ë¥˜: {e}")
        return

    # ë°ì´í„°ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ í—¤ë”ë§Œ ë³µêµ¬
    if not rows:
        header = ["sport", "id", "title", "summary"]
        try:
            ws.clear()
            ws.update("A1", [header])
        except Exception as e:
            await update.message.reply_text(f"ì‹œíŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            return
        reload_analysis_from_sheet()
        await update.message.reply_text(f"tomorrow ì‹œíŠ¸ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤. ({label})")
        return

    header = rows[0]
    data_rows = rows[1:]

    # sport ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì°¾ê¸° (ê¸°ë³¸ì€ 0)
    try:
        idx_sport = header.index("sport")
    except ValueError:
        idx_sport = 0

    kept_rows = [header]
    deleted_count = 0

    if sports_to_clear is None:
        # ì „ì²´ ì‚­ì œ (í—¤ë”ë§Œ ë‚¨ê¹€)
        deleted_count = len(data_rows)
    else:
        # í•´ë‹¹ ì¢…ëª©ë§Œ ì œì™¸í•˜ê³  ìœ ì§€
        for row in data_rows:
            sport_val = row[idx_sport] if len(row) > idx_sport else ""
            if sport_val in sports_to_clear:
                deleted_count += 1
                continue
            kept_rows.append(row)

    try:
        ws.clear()
        ws.update("A1", kept_rows)
    except Exception as e:
        await update.message.reply_text(f"ì‹œíŠ¸ ì“°ê¸° ì˜¤ë¥˜: {e}")
        return

    reload_analysis_from_sheet()

    if sports_to_clear is None:
        await update.message.reply_text(
            f"tomorrow ì‹œíŠ¸ì˜ ë¶„ì„ ë°ì´í„°ë¥¼ ì „ì²´ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤. (ì‚­ì œëœ í–‰: {deleted_count}ê°œ)"
        )
    else:
        await update.message.reply_text(
            f"tomorrow ì‹œíŠ¸ì—ì„œ {label} ë¶„ì„ ë°ì´í„°ë§Œ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤. (ì‚­ì œëœ í–‰: {deleted_count}ê°œ)"
        )

# âš½ ì¶•êµ¬ ê³„ì—´(í•´ì™¸ì¶•êµ¬ / Kë¦¬ê·¸ / Jë¦¬ê·¸)ë§Œ ì‚­ì œ
async def soccerclean(update: Update, context: ContextTypes.DEFAULT_TYPE):
    sports = {"í•´ì™¸ì¶•êµ¬", "Kë¦¬ê·¸", "Jë¦¬ê·¸"}
    await _analysis_clean_by_sports(
        update,
        context,
        sports_to_clear=sports,
        label="ì¶•êµ¬(í•´ì™¸ì¶•êµ¬/Kë¦¬ê·¸/Jë¦¬ê·¸)",
    )


# âš¾ ì•¼êµ¬ ê³„ì—´(í•´ì™¸ì•¼êµ¬ / KBO / NPB)ë§Œ ì‚­ì œ
async def baseballclean(update: Update, context: ContextTypes.DEFAULT_TYPE):
    sports = {"í•´ì™¸ì•¼êµ¬", "KBO", "NPB"}
    await _analysis_clean_by_sports(
        update,
        context,
        sports_to_clear=sports,
        label="ì•¼êµ¬(í•´ì™¸ì•¼êµ¬/KBO/NPB)",
    )


# ğŸ€ ë†êµ¬ë§Œ ì‚­ì œ
async def basketclean(update: Update, context: ContextTypes.DEFAULT_TYPE):
    # ë†êµ¬ ì „ì²´: ì˜ˆì „ 'ë†êµ¬' + ìƒˆ ë¼ë²¨ 'NBA', 'KBL'
    sports = {"ë†êµ¬", "NBA", "KBL"}
    await _clean_tomorrow_sheet(
        update,
        context,
        sports_to_clear=sports,
        label="ë†êµ¬(NBA/KBL)",
    )


# ğŸ ë°°êµ¬ë§Œ ì‚­ì œ
async def volleyclean(update: Update, context: ContextTypes.DEFAULT_TYPE):
    sports = {"ë°°êµ¬", "vë¦¬ê·¸"}
    await _analysis_clean_by_sports(
        update,
        context,
        sports_to_clear=sports,
        label="ë°°êµ¬/vë¦¬ê·¸",
    )


# ê¸°íƒ€ ì¢…ëª©ë§Œ ì‚­ì œ (ê¸°íƒ€ / ê¸°íƒ€ì¢… / ê¸°íƒ€ì¢…ëª©)
async def etcclean(update: Update, context: ContextTypes.DEFAULT_TYPE):
    sports = {"ê¸°íƒ€", "ê¸°íƒ€ì¢…", "ê¸°íƒ€ì¢…ëª©"}
    await _analysis_clean_by_sports(
        update,
        context,
        sports_to_clear=sports,
        label="ê¸°íƒ€ ì¢…ëª©",
    )


# tomorrow ì‹œíŠ¸ ì „ì²´ ë¶„ì„ ë°ì´í„° ì‚­ì œ (í—¤ë”ë§Œ ë‚¨ê¹€)
async def analysisclean(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await _analysis_clean_by_sports(
        update,
        context,
        sports_to_clear=None,
        label="ì „ì²´ ë¶„ì„",
    )

# ğŸ”¹ 4) /rollover â€“ ë‚´ì¼ ë¶„ì„ â†’ ì˜¤ëŠ˜ ë¶„ì„ìœ¼ë¡œ ë³µì‚¬
async def rollover(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not is_admin(update):
        await update.message.reply_text("ì´ ëª…ë ¹ì–´ëŠ” ê´€ë¦¬ìë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    client = get_gs_client()
    spreadsheet_id = os.getenv("SPREADSHEET_ID")

    if client and spreadsheet_id:
        try:
            sh = client.open_by_key(spreadsheet_id)

            sheet_today_name = os.getenv("SHEET_TODAY_NAME", "today")
            sheet_tomorrow_name = os.getenv("SHEET_TOMORROW_NAME", "tomorrow")

            ws_today = sh.worksheet(sheet_today_name)
            ws_tomorrow = sh.worksheet(sheet_tomorrow_name)

            rows = ws_tomorrow.get_all_values()

            if rows:
                ws_today.clear()
                ws_today.update("A1", rows)

                header = rows[0]
                ws_tomorrow.clear()
                ws_tomorrow.update("A1", [header])
            else:
                print("[GSHEET] tomorrow íƒ­ì— ë°ì´í„°ê°€ ì—†ì–´ ì‹œíŠ¸ ë¡¤ì˜¤ë²„ëŠ” ìƒëµí•©ë‹ˆë‹¤.")

        except Exception as e:
            print(f"[GSHEET] ë¡¤ì˜¤ë²„ ì¤‘ ì‹œíŠ¸ ë³µì‚¬ ì‹¤íŒ¨: {e}")

    else:
        print("[GSHEET] í´ë¼ì´ì–¸íŠ¸ ë˜ëŠ” SPREADSHEET_ID ì—†ìŒ â†’ ì‹œíŠ¸ ë¡¤ì˜¤ë²„ëŠ” ê±´ë„ˆëœ€.")

    reload_analysis_from_sheet()

    await update.message.reply_text(
        "âœ… ë¡¤ì˜¤ë²„ ì™„ë£Œ!\n"
        "êµ¬ê¸€ì‹œíŠ¸ 'tomorrow' íƒ­ ë‚´ìš©ì„ 'today' íƒ­ìœ¼ë¡œ ë³µì‚¬í–ˆê³ ,\n"
        "'tomorrow' íƒ­ì€ í—¤ë”ë§Œ ë‚¨ê¸°ê³  ì´ˆê¸°í™”í–ˆì–´.\n\n"
        "ì´ì œ ì˜¤ëŠ˜ ê²½ê¸° ë¶„ì„ì€ 'today' íƒ­ì—ì„œ, ë‚´ì¼ ê²½ê¸°ëŠ” 'tomorrow' íƒ­ì—ì„œ ì‘ì„±í•˜ë©´ ë¼."
    )


def simple_summarize(text: str, max_chars: int = 400) -> str:
    """
    ì•„ì£¼ ë‹¨ìˆœ ìš”ì•½: ë¬¸ì¥ ì‚¬ì´ ê³µë°± ì •ë¦¬ í›„,
    max_chars ì•ˆìª½ì—ì„œ 'ë‹¤.' ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ì„œ ë°˜í™˜.
    (Gemini ì˜¤ë¥˜ ì‹œ fallback ìš©)
    """
    if not text:
        return ""

    text = re.sub(r"\s+", " ", text).strip()

    if len(text) <= max_chars:
        return text

    cut = text.rfind("ë‹¤.", 0, max_chars)
    if cut != -1:
        return text[: cut + 2]

    return text[:max_chars] + "..."

# ğŸ”¹ OpenAI í´ë¼ì´ì–¸íŠ¸ (ìš”ì•½ìš©)
_openai_client = None

def get_openai_client():
    """
    OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•´ì„œ ëŒë ¤ì¤€ë‹¤.
    í‚¤ê°€ ì—†ìœ¼ë©´ Noneì„ ë¦¬í„´í•˜ê³ , ì—ëŸ¬ ì‹œ simple_summarize í´ë°±ì„ ì‚¬ìš©í•œë‹¤.
    """
    global _openai_client
    if _openai_client is not None:
        return _openai_client

    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    if not api_key:
        print("[OPENAI] OPENAI_API_KEY ë¯¸ì„¤ì • â†’ simple_summarize í´ë°± ì‚¬ìš©")
        return None

    try:
        _openai_client = OpenAI(api_key=api_key)
        print("[OPENAI] OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"[OPENAI] í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        _openai_client = None
    return _openai_client

# ğŸ”¹ mazgtv í™ë³´ ë¬¸êµ¬/í•´ì‹œíƒœê·¸ ê³µí†µ ì œê±°ìš© íŒ¨í„´
MAZ_REMOVE_PATTERNS = [
    # ê¸°ë³¸ í™ë³´ ë¬¸êµ¬
    r"ì‹¤ì‹œê°„\s*ìŠ¤í¬ì¸ ì¤‘ê³„",
    r"ìŠ¤í¬ì¸ \s*ì¤‘ê³„",
    r"ìŠ¤í¬ì¸ \s*ë¶„ì„",
    r"ìŠ¤í¬ì¸ \s*ì •ë³´",
    r"ë¼ì´ë¸Œ\s*ìŠ¤í¬ì¸ ì¤‘ê³„",
    r"ì‹¤ì‹œê°„\s*ë¬´ë£Œ\s*ì¤‘ê³„",
    r"ë¬´ë£Œ\s*ì¤‘ê³„",
    r"ë¬´ë£Œ\s*ìŠ¤í¬ì¸ ì¤‘ê³„",

    # ì‚¬ì´íŠ¸/ë¸Œëœë“œëª…
    r"ë§ˆì§•ê°€í‹°ë¹„",
    r"ë§ˆì§•ê°€\s*í‹°ë¹„",
    r"ë§ˆì§•ê°€TV",
    r"ë§ˆì§•ê°€\s*TV",
    r"ë§ˆì§•ê°€\s*í‹°ë¸Œì´",
    r"ë§ˆì§•ê°€\s*í‹°ë¹„\s*ë°”ë¡œê°€ê¸°",

    # ë°°ë„ˆ/ìœ ë„ ë¬¸êµ¬
    r"ë°°ë„ˆ\s*ë¬¸ì˜",
    r"ë°°ë„ˆ",
    r"ë§í¬\s*í´ë¦­",
    r"ë°”ë¡œê°€ê¸°",
    r"ìŠ¤í¬ì¸ ì¤‘ê³„\s*ë°”ë¡œê°€ê¸°",

    # í•´ì‹œíƒœê·¸
    r"#\S+",

    # ë‚ ì§œ/ì œëª© ë¼ì¸ (ì˜ˆ: 11ì›” 28ì¼ í”„ë¦¬ë·°, 11ì›” 28ì¼ ê²½ê¸° ë¶„ì„)
    r"11ì›”\s*\d{1,2}\s*[^\n]{0,30}",
    r"\d{1,2}ì›”\s*\d{1,2}ì¼\s*[^\n]{0,30}",

    # ì œëª© íŒ¨í„´ (ì¤‘ê³„ / ë¶„ì„)
    r"[ê°€-í£A-Za-z0-9 ]+ ì¤‘ê³„",
    r"[ê°€-í£A-Za-z0-9 ]+ ë¶„ì„",
    r"[ê°€-í£A-Za-z0-9 ]+ í”„ë¦¬ë·°",

    # ì„¹ì…˜ ì œëª©ë“¤
    r"í”„ë¦¬ë·°",
    r"í•µì‹¬\s*í¬ì¸íŠ¸",
    r"í•µì‹¬\s*í¬ì¸íŠ¸\s*ì •ë¦¬",
    r"ìŠ¹ë¶€\s*ì˜ˆì¸¡",
    r"ë² íŒ…\s*ê°•ë„",
    r"ë§ˆë¬´ë¦¬\s*ì½”ë©˜íŠ¸",
    r"ë§ˆë¬´ë¦¬\s*ì •ë¦¬",

    # ì‚¬ì´íŠ¸ë¡œ ìœ ë„í•˜ëŠ” ê¼¬ë¦¬ ë¬¸êµ¬
    r"ì—ì„œ\s*í™•ì¸í•˜ì„¸ìš”[^\n]*",

    # í”½ ë¼ì¸ (ìŠ¹/ë¬´/íŒ¨, í•¸ë””, ì–¸ë”ì˜¤ë²„)
    r"\[ìŠ¹/ë¬´/íŒ¨\][^\n]+",
    r"\[í•¸ë””\][^\n]+",
    r"\[ì–¸ë”ì˜¤ë²„\][^\n]+",
    r"ìŠ¹íŒ¨\s*ì¶”ì²œ[^\n]*",
    r"ì¶”ì²œ\s*í”½[^\n]*",

    # ì´ëª¨ì§€/ì•„ì´ì½˜ë¥˜
    r"âœ…",
    r"â­•",
    r"âš ï¸",
    r"â­+",
    r"ğŸ”¥",
    r"ğŸ‘‰",
]

def clean_maz_text(text: str) -> str:
    """
    mazgtv ì›ë¬¸/ìš”ì•½ì—ì„œ í™ë³´ ë¬¸êµ¬, í•´ì‹œíƒœê·¸ ë“±ì„ ì œê±°í•˜ê³ 
    ê³µë°±ì„ ì •ë¦¬í•´ì„œ ëŒë ¤ì¤€ë‹¤.
    """
    if not text:
        return ""
    for pattern in MAZ_REMOVE_PATTERNS:
        text = re.sub(pattern, "", text, flags=re.IGNORECASE)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def extract_mmdd_from_kickoff(kickoff: str) -> tuple[int | None, int | None]:
    """
    '11-28 (ê¸ˆ) 02:45' ê°™ì€ ë¬¸ìì—´ì—ì„œ (month, day)ë¥¼ ë½‘ëŠ”ë‹¤.
    ë‹¤ë¥¸ í¬ë§·(ì˜ˆ: '11ì›” 28ì¼ 02:45')ë„ ëŒ€ë¹„í•´ì„œ ì •ê·œì‹ ë‘ ê°œë¥¼ ì‹œë„.
    """
    if not kickoff:
        return (None, None)

    text = kickoff.strip()

    # 1) 11-28 (ê¸ˆ) 02:45 í˜•íƒœ
    m = re.search(r"(\d{1,2})\s*-\s*(\d{1,2})", text)
    if not m:
        # 2) 11ì›” 28ì¼ (ê¸ˆ) 02:45 í˜•íƒœ
        m = re.search(r"(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼", text)

    if not m:
        return (None, None)

    try:
        month = int(m.group(1))
        day = int(m.group(2))
        return (month, day)
    except ValueError:
        return (None, None)

def ensure_team_line_breaks(body: str, home_team: str, away_team: str) -> str:
    """
    ìš”ì•½ ë³¸ë¬¸ì—ì„œ 'í™ˆíŒ€: ... ì›ì •íŒ€:' ì´ í•œ ì¤„ì— ë¶™ì–´ ìˆì„ ë•Œ
    í™ˆíŒ€ ë¸”ë¡ / ì›ì •íŒ€ ë¸”ë¡ / ğŸ¯ í”½ ì‚¬ì´ì— ë¹ˆ ì¤„ì„ ê°•ì œë¡œ ë„£ì–´ ì¤€ë‹¤.
    """
    if not body:
        return body

    body = body.replace("\r\n", "\n")

    # í™ˆíŒ€: ... ì›ì •íŒ€: ì´ í•œ ì¤„ì— ë¶™ì–´ ìˆìœ¼ë©´ ê°•ì œ ë¶„ë¦¬
    if home_team and away_team:
        pattern = rf"({re.escape(home_team)}:[^\n]+)\s+({re.escape(away_team)}:)"
        body = re.sub(pattern, r"\1\n\n\2", body)

    # ì›ì •íŒ€: ... ğŸ¯ í”½ ë¶™ì–´ ìˆìœ¼ë©´ ë¶„ë¦¬
    if away_team:
        pattern2 = rf"({re.escape(away_team)}:[^\n]+)\s+ğŸ¯\s*í”½"
        body = re.sub(pattern2, r"\1\n\nğŸ¯ í”½", body)

    # ğŸ¯ í”½ ë¼ì¸ì„ í•­ìƒ ë‹¨ë… ì¤„ë¡œ
    body = re.sub(r"\s*ğŸ¯\s*í”½\s*", "\n\nğŸ¯ í”½\n", body)

    # ì—¬ëŸ¬ ê³µë°± ì •ë¦¬
    body = re.sub(r"[ \t]+", " ", body)
    return body.strip()


def _postprocess_analysis_body(body: str, home_label: str, away_label: str) -> str:
    """
    - íŒ€ë³„ ë¸”ë¡ ì‚¬ì´ ì¤„ë°”ê¿ˆ ê°•ì œ
    - ğŸ¯ í”½ ì•„ë˜ëŠ” 'â¡' ë¡œ ì‹œì‘í•˜ëŠ” 3ì¤„ë§Œ ë‚¨ê¸°ê¸°
    """
    body = ensure_team_line_breaks(body, home_label, away_label)

    if "ğŸ¯ í”½" in body:
        head, tail = body.split("ğŸ¯ í”½", 1)
        lines = [ln.strip() for ln in tail.splitlines() if ln.strip()]

        # â¡ ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ë§Œ ê³¨ë¼ì„œ ìµœëŒ€ 3ì¤„
        picks = [ln for ln in lines if ln.startswith("â¡")]
        picks = picks[:3]

        if picks:
            tail_norm = "ğŸ¯ í”½\n" + "\n".join(picks)
            body = head.rstrip() + "\n\n" + tail_norm
        else:
            # í”½ì´ ì´ìƒí•˜ê²Œ ë‚˜ì˜¤ë©´ ê·¸ëƒ¥ ì˜ë¼ë²„ë¦¼
            body = head.rstrip()

    return body.strip()


def summarize_analysis_with_gemini(
    full_text: str,
    *,
    league: str = "í•´ì™¸ì¶•êµ¬",
    home_team: str = "",
    away_team: str = "",
    max_chars: int = 900,
) -> tuple[str, str]:
    """
    ğŸ‘‰ ì´ì œëŠ” OpenAI(gpt-4.1-mini)ë¥¼ ì‚¬ìš©í•´ì„œ
       'ì œëª© + íŒ€ë³„ ìš”ì•½ + ğŸ¯ í”½' í˜•ì‹ìœ¼ë¡œ ê²½ê¸° ë¶„ì„ì„ ìƒì„±í•œë‹¤.
    """
    client_oa = get_openai_client()

    # ê¸°ë³¸ ì œëª©
    if home_team and away_team:
        base_title = f"[{league}] {home_team} vs {away_team} ê²½ê¸° ë¶„ì„"
    else:
        base_title = f"[{league}] í•´ì™¸ì¶•êµ¬ ê²½ê¸° ë¶„ì„"

    home_label = home_team or "í™ˆíŒ€"
    away_label = away_team or "ì›ì •íŒ€"

    # ì›ë¬¸ ì •ë¦¬
    full_text_clean = clean_maz_text(full_text or "").strip()
    if len(full_text_clean) > 7000:
        full_text_clean = full_text_clean[:7000]

    # OpenAI í‚¤ ì—†ìœ¼ë©´ ê°„ë‹¨ í´ë°±
    if not client_oa:
        core = simple_summarize(full_text_clean, max_chars=max_chars)
        body = (
            f"{home_label}:\n{core}\n\n"
            "ğŸ¯ í”½\n"
            "â¡ï¸ ê²½ê¸° íë¦„ ì°¸ê³ ìš© í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n"
            "â¡ï¸ ì‹¤ì œ ë² íŒ… ì „ ë¼ì¸Â·ë¶€ìƒ ì •ë³´ë¥¼ ë°˜ë“œì‹œ ë‹¤ì‹œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "â¡ï¸ ì„¸ë¶€ ì¶”ì²œí”½ì€ ë³„ë„ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤."
        )
        return (base_title or "[ê²½ê¸° ë¶„ì„]", body)

    # â”€â”€ í”„ë¡¬í”„íŠ¸ â”€â”€
    prompt = f"""
ë‹¤ìŒì€ í•´ì™¸ì¶•êµ¬ ê²½ê¸° ë¶„ì„ ì›ë¬¸ì´ë‹¤.
ì „ì²´ ë‚´ìš©ì„ ì´í•´í•œ ë’¤, ì•„ë˜ì— ì œì‹œí•œ â€˜ì—„ê²©í•œ í˜•ì‹â€™ ê·¸ëŒ€ë¡œ ì‘ì„±í•˜ë¼.
ì›ë¬¸ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ê³  ë°˜ë“œì‹œ ì¬ì‘ì„±í•˜ê³ , í˜•ì‹ì—ì„œ ë²—ì–´ë‚˜ëŠ” í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆë¼.

ì¶œë ¥ í˜•ì‹ì€ ì•„ë˜ë¥¼ ì •í™•íˆ ì§€ì¼œë¼:

ì œëª©: [ë¦¬ê·¸] í™ˆíŒ€ vs ì›ì •íŒ€ ê²½ê¸° ë¶„ì„
ìš”ì•½:
{home_label}:
- ë¬¸ì¥1
- ë¬¸ì¥2
(ë¬¸ì¥ ìˆ˜ëŠ” 2~3ê°œ, ë°˜ë“œì‹œ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)

{away_label}:
- ë¬¸ì¥1
- ë¬¸ì¥2
(ë¬¸ì¥ ìˆ˜ëŠ” 2~3ê°œ)

ğŸ¯ í”½
â¡ï¸ í™ˆíŒ€/ì›ì •íŒ€ ìŠ¹ ê´€ë ¨ 1ì¤„
â¡ï¸ í•¸ë”” ê´€ë ¨ 1ì¤„
â¡ï¸ ì˜¤ë²„/ì–¸ë” ê´€ë ¨ 1ì¤„

â— ì ˆëŒ€ ê¸ˆì§€:
- í”½ ì„¹ì…˜ì— ì„¤ëª…ë¬¸ ì¶”ê°€ ê¸ˆì§€
- í”½ì„ 3ì¤„ ì´ˆê³¼í•˜ê±°ë‚˜ 3ì¤„ë³´ë‹¤ ì ê²Œ ì“°ëŠ” ê²ƒ ê¸ˆì§€
- {home_label}/{away_label} ë¸”ë¡ ì‚¬ì´ ì¤„ë°”ê¿ˆ ëˆ„ë½ ê¸ˆì§€
- íŒ€ ì´ë¦„ ì—†ì´ ë¶„ì„ ì‹œì‘ ê¸ˆì§€
- ğŸ¯ í”½ ìœ„ì— ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì¶œë ¥ ê¸ˆì§€
- í˜•ì‹ê³¼ ë‹¤ë¥¸ ì—¬ë¶„ ë¬¸ì¥ ì¶œë ¥ ê¸ˆì§€

ì•„ë˜ëŠ” ë¦¬ê·¸/íŒ€ ì •ë³´ë‹¤.
ë¦¬ê·¸: {league}
í™ˆíŒ€: {home_label}
ì›ì •íŒ€: {away_label}

===== ê²½ê¸° ë¶„ì„ ì›ë¬¸ =====
{full_text_clean}
""".strip()

    try:
        resp = client_oa.chat.completions.create(
            model=os.getenv("OPENAI_MODEL_ANALYSIS", "gpt-4.1-mini"),
            messages=[
                {
                    "role": "system",
                    "content": (
                        "ë„ˆëŠ” ì¶•êµ¬ ê²½ê¸° ë¶„ì„ì„ ìš”ì•½í•´ì„œ ì •ë¦¬í•˜ëŠ” í•œêµ­ì–´ ì „ë¬¸ê°€ë‹¤. "
                        "ë¬¸ì¥ì€ ê°„ê²°í•˜ê³  ì§ì„¤ì ìœ¼ë¡œ ì“°ê³ , í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€í‚¨ë‹¤."
                    ),
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.4,
            max_completion_tokens=700,
        )
        text_out = (resp.choices[0].message.content or "").strip()
        if not text_out:
            raise ValueError("empty response from OpenAI (analysis)")

        # ì œëª© / ìš”ì•½ ë¶„ë¦¬
        m_title = re.search(r"ì œëª©\s*[:ï¼š]\s*(.+)", text_out)
        m_body = re.search(r"ìš”ì•½\s*[:ï¼š]\s*(.+)", text_out, flags=re.S)

        new_title = (m_title.group(1).strip() if m_title else "").strip()
        body = (m_body.group(1).strip() if m_body else text_out).strip()

        if not new_title:
            new_title = base_title or "[ê²½ê¸° ë¶„ì„]"

        # ì œëª©ì´ ë³¸ë¬¸ì— ë˜ ë°˜ë³µë˜ë©´ ì˜ë¼ë‚´ê¸°
        body = remove_title_prefix(new_title, body)

        # í˜•ì‹ ê°•ì œ í›„ì²˜ë¦¬ (íŒ€ë³„ ì¤„ë°”ê¿ˆ + í”½ 3ì¤„)
        body = _postprocess_analysis_body(body, home_label, away_label)

        if len(body) > max_chars + 200:
            body = body[: max_chars + 200]

        return (new_title, body)

    except Exception as e:
        print(f"[OPENAI][ANALYSIS] ì‹¤íŒ¨ â†’ simple_summarize í´ë°±: {e}")
        core = simple_summarize(full_text_clean, max_chars=max_chars)
        body = (
            f"{home_label}:\n{core}\n\n"
            "ğŸ¯ í”½\n"
            "â¡ï¸ ê²½ê¸° íë¦„ ì°¸ê³ ìš© í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n"
            "â¡ï¸ ì‹¤ì œ ë² íŒ… ì „ ë¼ì¸Â·ë¶€ìƒ ì •ë³´ë¥¼ ë°˜ë“œì‹œ ë‹¤ì‹œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "â¡ï¸ ì„¸ë¶€ ì¶”ì²œí”½ì€ ë³„ë„ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤."
        )
        return (base_title or "[ê²½ê¸° ë¶„ì„]", body)
        
def rewrite_for_site_openai(
    full_text: str,
    *,
    league: str,
    home_team: str,
    away_team: str,
    max_chars: int = 4500,
) -> tuple[str, str]:
    """
    ì‚¬ì´íŠ¸ ê²Œì‹œìš©: ì›ë¬¸(full_text) ê¸°ë°˜ ì„œìˆ í˜• ì¬ì‘ì„±.
    - í—ˆêµ¬/ì¶”ì¸¡ ê¸ˆì§€
    - ì›ë¬¸ê³¼ ì–´ê¸‹ë‚˜ëŠ” ì •ë³´ ì¶”ê°€ ê¸ˆì§€
    - 'ìŠ¤í¬ì¸ ë¶„ì„', 'ê³ íŠ¸í‹°ë¹„' í‚¤ì›Œë“œ ìì—°ìŠ¤ëŸ½ê²Œ 1~2íšŒ ì‚½ì…
    """
    text = (full_text or "").strip()
    if not text or len(text) < 200:
        raise ValueError("ì›ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ(ì‚¬ì´íŠ¸ìš© ìƒì„± ìŠ¤í‚µ)")

    client_oa = get_openai_client()
    if not client_oa:
        raise ValueError("OPENAI_API_KEY ì—†ìŒ(ì‚¬ì´íŠ¸ìš© ìƒì„± ìŠ¤í‚µ)")

    base_title = f"[{league}] {home_team} vs {away_team} ê²½ê¸° ë¶„ì„".strip()

    # ë„ˆë¬´ ê¸¸ë©´ ì»·
    if len(text) > 9000:
        text = text[:9000]

    prompt = f"""
ë‹¤ìŒì€ ìŠ¤í¬ì¸  ê²½ê¸° ë¶„ì„ ì›ë¬¸ì´ë‹¤.
ì›ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œë§Œ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì¬ì‘ì„±í•˜ë¼.
ì ˆëŒ€ë¡œ ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€/ì¶”ì¸¡/ë‹¨ì •í•˜ì§€ ë§ˆë¼.

ìš”êµ¬ì‚¬í•­:
- ì œëª© 1ê°œ + ë³¸ë¬¸(ì„œìˆ í˜•)ë§Œ ì‘ì„±
- ë³¸ë¬¸ì€ 6~14ë¬¸ë‹¨ ë‚´ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ êµ¬ì„±(ì¤„ë°”ê¿ˆ ìœ ì§€)
- íŒ€ ì „ë ¥/í•µì‹¬ í¬ì¸íŠ¸/ê²½ê¸° íë¦„ ì „ë§ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬
- 'ìŠ¤í¬ì¸ ë¶„ì„' í‚¤ì›Œë“œë¥¼ ë³¸ë¬¸ì— 1~2íšŒ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
- 'ê³ íŠ¸í‹°ë¹„' í‚¤ì›Œë“œë¥¼ ë³¸ë¬¸ì— 1íšŒ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
- ë² íŒ… í”½/ë°°ë‹¹/í™•ë¥ /ìŠ¹ë¶€ ë‹¨ì •ì€ ì“°ì§€ ë§ê³ , ê°€ëŠ¥ì„±/íë¦„ ì¤‘ì‹¬ìœ¼ë¡œë§Œ
- ì›ë¬¸ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ ê²ƒ(ì¬ì‘ì„±)

ì¶œë ¥ í˜•ì‹(ë°˜ë“œì‹œ):
ì œëª©: ...
ë³¸ë¬¸:
... (ì—¬ê¸°ë¶€í„° ë³¸ë¬¸)

ë¦¬ê·¸: {league}
í™ˆíŒ€: {home_team}
ì›ì •íŒ€: {away_team}

===== ì›ë¬¸ =====
{text}
""".strip()

    resp = client_oa.chat.completions.create(
        model=os.getenv("OPENAI_MODEL_SITE", "gpt-4.1-mini"),
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ìŠ¤í¬ì¸  ê²½ê¸° ë¶„ì„ ì›ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì¬ì‘ì„±í•˜ëŠ” í•œêµ­ì–´ ì—ë””í„°ë‹¤. í—ˆêµ¬ë¥¼ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.35,
        max_completion_tokens=1200,
    )

    out = (resp.choices[0].message.content or "").strip()
    if not out:
        raise ValueError("site rewrite empty")

    # íŒŒì‹±
    title = base_title
    body = out

    m1 = re.search(r"ì œëª©\s*[:ï¼š]\s*(.+)", out)
    m2 = re.search(r"ë³¸ë¬¸\s*[:ï¼š]\s*(.+)", out, flags=re.S)
    if m1:
        title = m1.group(1).strip()
    if m2:
        body = m2.group(1).strip()

    # ê¸¸ì´ ì œí•œ
    if len(body) > max_chars:
        body = body[:max_chars].rstrip()

    return title, body

    except Exception as e:
        print(f"[OPENAI][SITE] ì‹¤íŒ¨ â†’ simple_summarize í´ë°±: {e}")
        body = simple_summarize(text, max_chars=min(max_chars, 1200))
        return (base_title, body)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë‰´ìŠ¤ìš© Gemini ìš”ì•½ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def summarize_with_gemini(full_text: str, orig_title: str = "", max_chars: int = 400) -> tuple[str, str]:
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ìš© ìš”ì•½ í•¨ìˆ˜.
    ì´ì œ OpenAI(gpt-4.1-mini)ë¥¼ ì‚¬ìš©í•´ì„œ
    'ì œëª©: ... / ìš”ì•½: ...' í˜•ì‹ìœ¼ë¡œ ë¦¬ë¼ì´íŒ…í•œë‹¤.
    """
    client_oa = get_openai_client()
    trimmed = (full_text or "").strip()
    if len(trimmed) > 6000:
        trimmed = trimmed[:6000]

    # í‚¤ ì—†ìœ¼ë©´ í´ë°±
    if not client_oa:
        print("[OPENAI][NEWS] í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ â†’ simple_summarize ì‚¬ìš©")
        fb_summary = simple_summarize(trimmed, max_chars=max_chars)
        fb_summary = clean_maz_text(fb_summary)
        return (orig_title or "[ì œëª© ì—†ìŒ]", fb_summary)

    prompt = (
        "ë‹¤ìŒì€ ìŠ¤í¬ì¸  ë‰´ìŠ¤ ê¸°ì‚¬ ì›ë¬¸ê³¼ ê¸°ì¡´ ì œëª©ì´ë‹¤.\n"
        "ì „ì²´ ë‚´ìš©ì„ ì´í•´í•œ ë’¤, ìƒˆë¡œìš´ í•œêµ­ì–´ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ 1ê°œì™€ 2~3ë¬¸ì¥ì§œë¦¬ ìš”ì•½ì„ ì‘ì„±í•´ì¤˜.\n"
        "ê¸°ì‚¬ ì•ë¶€ë¶„ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ ê²ƒ.\n"
        f"ìš”ì•½ ê¸¸ì´ëŠ” ê³µë°± í¬í•¨ {max_chars}ì ë‚´ì™¸.\n"
        "\n"
        "ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´:\n"
        "ì œëª©: (ì—¬ê¸°ì— ìƒˆ ì œëª©)\n"
        "ìš”ì•½: (ì—¬ê¸°ì— ìš”ì•½ë¬¸)\n"
        "ê·¸ ì™¸ì˜ ë¬¸ì¥ì€ ì¶œë ¥í•˜ì§€ ë§ˆ.\n"
        "\n"
        "===== ê¸°ì¡´ ì œëª© =====\n"
        f"{orig_title}\n"
        "\n"
        "===== ê¸°ì‚¬ ì›ë¬¸ =====\n"
        f"{trimmed}\n"
    )

    try:
        resp = client_oa.chat.completions.create(
            model=os.getenv("OPENAI_MODEL_NEWS", "gpt-4.1-mini"),
            messages=[
                {
                    "role": "system",
                    "content": "ë„ˆëŠ” ìŠ¤í¬ì¸  ë‰´ìŠ¤ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ëŠ” í•œêµ­ì–´ ê¸°ìë‹¤. "
                               "í˜•ì‹ì„ ì •í™•íˆ ì§€í‚¤ê³ , ì¤‘ë³µ í‘œí˜„ì€ ì¤„ì¸ë‹¤.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.5,
            max_completion_tokens=450,
        )
        text_out = (resp.choices[0].message.content or "").strip()
        if not text_out:
            raise ValueError("empty response from OpenAI (news)")

        new_title = ""
        summary = ""
        for line in text_out.splitlines():
            line = line.strip()
            if line.startswith("ì œëª©:"):
                new_title = line[len("ì œëª©:"):].strip(" ï¼š:")
            elif line.startswith("ìš”ì•½:"):
                summary = line[len("ìš”ì•½:"):].strip(" ï¼š:")

        if not summary:
            summary = text_out

        if len(summary) > max_chars + 100:
            summary = summary[: max_chars + 100]

        if not new_title:
            new_title = orig_title or "[ì œëª© ì—†ìŒ]"

        summary = clean_maz_text(summary)
        return (new_title, summary)

    except Exception as e:
        print(f"[OPENAI][NEWS] ìš”ì•½ ì‹¤íŒ¨ â†’ simple_summarizeë¡œ í´ë°±: {e}")
        fb_summary = simple_summarize(trimmed, max_chars=max_chars)
        fb_summary = clean_maz_text(fb_summary)
        return (orig_title or "[ì œëª© ì—†ìŒ]", fb_summary)

def extract_main_text_from_html(soup: BeautifulSoup) -> str:
    """
    mazgtv ë¶„ì„ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ ì˜ ë½‘ì•„ì„œ ë¦¬í„´.
    HTML êµ¬ì¡°ë¥¼ ì •í™•íˆ ëª¨ë¥¼ ë•Œë¥¼ ëŒ€ë¹„í•´ì„œ ì—¬ëŸ¬ í›„ë³´ ì…€ë ‰í„°ë¥¼ ì‹œë„í•˜ê³ ,
    ê·¸ë˜ë„ ì—†ìœ¼ë©´ body ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©.
    """
    # ê´‘ê³ /ìŠ¤í¬ë¦½íŠ¸ ì œê±°
    for bad in soup.select("script, style, noscript"):
        try:
            bad.decompose()
        except Exception:
            pass

    candidates = [
        "div.ql-editor",      # ì—ë””í„° ë³¸ë¬¸ì¼ ë•Œ ìì£¼ ì“°ëŠ” í´ë˜ìŠ¤
        "div.v-card__text",   # vuetify ì¹´ë“œ ë³¸ë¬¸
        "div.article-body",
        "div.view-cont",
        "div#content",
        "article",
        "main",
    ]

    for sel in candidates:
        el = soup.select_one(sel)
        if not el:
            continue
        text = el.get_text("\n", strip=True)
        if len(text) >= 200:   # ë„ˆë¬´ ì§§ìœ¼ë©´ ë³¸ë¬¸ì´ ì•„ë‹ ê°€ëŠ¥ì„±
            return re.sub(r"\s+", " ", text).strip()

    # í›„ë³´ë“¤ì—ì„œ ëª» ì°¾ìœ¼ë©´ bodyFallback
    body = soup.body or soup
    text = body.get_text("\n", strip=True)
    return re.sub(r"\s+", " ", text).strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Daum harmony API ê³µí†µ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def fetch_daum_news_json(client: httpx.AsyncClient, category_id: str, size: int = 20) -> list[dict]:
    """
    ë‹¤ìŒ ìŠ¤í¬ì¸  harmony APIì—ì„œ íŠ¹ì • ì¹´í…Œê³ ë¦¬ IDì˜ ë‰´ìŠ¤ JSON ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¨ë‹¤.
    (í•´ì™¸ì¶•êµ¬, KBO, í•´ì™¸ì•¼êµ¬, ë†êµ¬, ë°°êµ¬ ê³µí†µ)
    """
    if not category_id:
        return []

    base_url = "https://sports.daum.net/media-api/harmony/contents.json"

    today_kst = get_kst_now().date()
    ymd = today_kst.strftime("%Y%m%d")
    create_dt = f"{ymd}000000~{ymd}235959"

    discovery_tag_value = json.dumps({
        "group": "media",
        "key": "defaultCategoryId3",
        "value": str(category_id),
    }, ensure_ascii=False)

    params = {
        "page": 0,
        "consumerType": "HARMONY",
        "status": "SERVICE",
        "createDt": create_dt,
        "size": size,
        "discoveryTag[0]": discovery_tag_value,
    }

    r = await client.get(base_url, params=params, timeout=10.0)
    r.raise_for_status()
    data = r.json()

    contents = None
    if isinstance(data, dict):
        contents = data.get("contents")
        if contents is None:
            inner = data.get("data") or data.get("result") or data.get("body")
            if isinstance(inner, dict):
                contents = inner.get("contents") or inner.get("list") or inner.get("items")
    elif isinstance(data, list):
        contents = data

    if not contents:
        print("[CRAWL][DAUM] JSON êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìµœìƒìœ„ í‚¤:",
              list(data.keys()) if isinstance(data, dict) else type(data))
        return []

    return contents


async def fetch_article_body(client: httpx.AsyncClient, url: str) -> str:
    """
    (ì˜ˆì „ ë„¤ì´ë²„ìš©) ë‰´ìŠ¤ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ.
    í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ ë‚¨ê²¨ë‘ .
    """
    try:
        r = await client.get(url, timeout=10.0, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
    except Exception as e:
        print(f"[CRAWL][ARTICLE] ìš”ì²­ ì‹¤íŒ¨: {url} / {e}")
        return ""

    soup = BeautifulSoup(r.text, "html.parser")

    body = soup.select_one("#newsEndContents")
    if body:
        return body.get_text("\n", strip=True)

    body = soup.select_one("#newsEndBody")
    if body:
        return body.get_text("\n", strip=True)

    body = soup.select_one("#dic_area")
    if body:
        return body.get_text("\n", strip=True)

    print(f"[CRAWL][ARTICLE] ë³¸ë¬¸ ì…€ë ‰í„° ë§¤ì¹˜ ì‹¤íŒ¨: {url}")
    return ""


async def crawl_daum_news_common(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    *,
    category_id: str,
    sport_label: str,
    max_articles: int = 10,
):
    """
    Daum harmony API + HTML ë³¸ë¬¸ì„ ì´ìš©í•´ ë‰´ìŠ¤ í¬ë¡¤ë§ í›„
    êµ¬ê¸€ì‹œíŠ¸ news íƒ­ì— ì €ì¥í•˜ëŠ” ê³µí†µ í•¨ìˆ˜.
    """
    if not is_admin(update):
        await update.message.reply_text("ì´ ëª…ë ¹ì–´ëŠ” ê´€ë¦¬ìë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    if not category_id:
        await update.message.reply_text(
            f"{sport_label} ì¹´í…Œê³ ë¦¬ IDê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
            "ì½”ë“œ ìƒë‹¨ DAUM_CATEGORY_IDS ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”."
        )
        return

    await update.message.reply_text(
        f"ë‹¤ìŒìŠ¤í¬ì¸  {sport_label} ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”..."
    )

    try:
        async with httpx.AsyncClient(
            headers={"User-Agent": "Mozilla/5.0"},
            follow_redirects=True,
        ) as client:
            contents = await fetch_daum_news_json(client, category_id, size=max_articles)

            if not contents:
                await update.message.reply_text(f"{sport_label} JSON ë°ì´í„°ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return

            articles: list[dict] = []

            # 1) JSONì—ì„œ ì œëª© + ê¸°ì‚¬ URL ì¶”ì¶œ
            for item in contents:
                if not isinstance(item, dict):
                    continue

                title = (
                    item.get("title")
                    or item.get("contentTitle")
                    or item.get("headline")
                    or item.get("name")
                )

                url = (
                    item.get("contentUrl")
                    or item.get("permalink")
                    or item.get("url")
                    or item.get("link")
                )

                if not title or not url:
                    continue

                title = str(title).strip()
                url = str(url).strip()

                if url.startswith("/"):

                    url = urljoin("https://sports.daum.net", url)

                articles.append({"title": title, "link": url})

                if len(articles) >= max_articles:
                    break

            if not articles:
                await update.message.reply_text(
                    f"JSONì€ ë°›ì•˜ì§€ë§Œ, {sport_label} ì œëª©/URL ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                )
                return

            # 2) ê° ê¸°ì‚¬ í˜ì´ì§€ ë“¤ì–´ê°€ì„œ ë³¸ë¬¸ í¬ë¡¤ë§ + ìš”ì•½
            for art in articles:
                try:
                    r2 = await client.get(art["link"], timeout=10.0)
                    r2.raise_for_status()
                    s2 = BeautifulSoup(r2.text, "html.parser")

                    body_el = (
                        s2.select_one("div#harmonyContainer")
                        or s2.select_one("section#article-view-content-div")
                        or s2.select_one("div.article_view")
                        or s2.select_one("div#mArticle")
                        or s2.find("article")
                        or s2.body
                    )

                    raw_body = ""
                    if body_el:
                        # ì´ë¯¸ì§€ ì„¤ëª… ìº¡ì…˜ ì œê±°
                        try:
                            for cap in body_el.select(
                                "figcaption, .txt_caption, .photo_desc, .caption, "
                                "em.photo_desc, span.caption, p.caption"
                            ):
                                try:
                                    cap.extract()
                                except Exception:
                                    pass
                        except Exception:
                            # selectê°€ ì•ˆ ë˜ëŠ” ê²½ìš°ëŠ” ê·¸ëƒ¥ ë¬´ì‹œ
                            pass

                        raw_body = body_el.get_text("\n", strip=True)
                    
                    clean_text = clean_daum_body_text(raw_body)
                    clean_text = remove_title_prefix(art["title"], clean_text)
                    
                    # âœ… Geminië¡œ "ìƒˆ ì œëª© + ìš”ì•½" ìƒì„± (400ì ë‚´ì™¸)
                    new_title, new_summary = summarize_with_gemini(
                        clean_text,
                        orig_title=art["title"],
                        max_chars=400,
                    )

                    art["title"] = new_title
                    art["summary"] = new_summary

                except Exception as e:
                    print(f"[CRAWL][DAUM] ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨ ({art['link']}): {e}")
                    # í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œì—ë„ ìµœì†Œí•œ ë­”ê°€ ë„£ì–´ë‘ê¸°
                    art["summary"] = "(ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨)"

    except Exception as e:
        await update.message.reply_text(f"ìš”ì²­ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return

    # 3) êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥
    client_gs = get_gs_client()
    spreadsheet_id = os.getenv("SPREADSHEET_ID")

    if not (client_gs and spreadsheet_id):
        await update.message.reply_text(
            "êµ¬ê¸€ì‹œíŠ¸ ì„¤ì •(GOOGLE_SERVICE_KEY ë˜ëŠ” SPREADSHEET_ID)ì´ ì—†ì–´ ì‹œíŠ¸ì— ì €ì¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        )
        return

    try:
        sh = client_gs.open_by_key(spreadsheet_id)
        ws = sh.worksheet(os.getenv("SHEET_NEWS_NAME", "news"))
    except Exception as e:
        await update.message.reply_text(f"ë‰´ìŠ¤ ì‹œíŠ¸ë¥¼ ì—´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
        return

    rows_to_append = []
    for art in articles:
        rows_to_append.append([
            sport_label,      # sport
            "",               # id
            art["title"],     # title
            art["summary"],   # summary
        ])

    try:
        ws.append_rows(rows_to_append, value_input_option="RAW")
    except Exception as e:
        await update.message.reply_text(f"ì‹œíŠ¸ ì“°ê¸° ì˜¤ë¥˜: {e}")
        return

    await update.message.reply_text(
        f"ë‹¤ìŒìŠ¤í¬ì¸  {sport_label} ë‰´ìŠ¤ {len(rows_to_append)}ê±´ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n"
        "/syncsheet ë¡œ í…”ë ˆê·¸ë¨ ë©”ë‰´ë¥¼ ê°±ì‹ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ mazgtv ë¶„ì„ ê³µí†µ (ë‚´ì¼ ê²½ê¸° â†’ today/tomorrow ì‹œíŠ¸, JSON/API ë²„ì „) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MAZ_LIST_API = "https://mazgtv1.com/api/board/list"
# ìƒì„¸ API ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ë©´ ë¨
MAZ_DETAIL_API_TEMPLATE = "https://mazgtv1.com/api/board/{board_id}"


def _parse_game_start_date(game_start_at: str) -> date | None:
    """
    '2025-11-28T05:00:00' ê°™ì€ ë¬¸ìì—´ì—ì„œ ë‚ ì§œ(date)ë§Œ ë½‘ëŠ”ë‹¤.
    """
    if not game_start_at:
        return None
    try:
        # ë’¤ì— íƒ€ì„ì¡´ì´ ë¶™ì–´ ìˆì–´ë„ ì• 19ìë¦¬ê¹Œì§€ë§Œ ì˜ë¼ì„œ íŒŒì‹±
        s = game_start_at[:19]
        dt = datetime.strptime(s, "%Y-%m-%dT%H:%M:%S")
        return dt.date()
    except Exception:
        return None

from datetime import date  # íŒŒì¼ ìœ„ìª½ì— ì´ë¯¸ ìˆì„ ìˆ˜ë„ ìˆìŒ

def detect_game_date_from_item(item: dict, target_date: date) -> date | None:
    """
    mazgtv ë¦¬ìŠ¤íŠ¸ JSON í•œ ê±´(item) ì „ì²´ë¥¼ í›‘ìœ¼ë©´ì„œ
    target_date ì™€ 'ê°™ì€ ë‚ ì§œ'ê°€ ë“¤ì–´ìˆëŠ”ì§€ ì°¾ëŠ”ë‹¤.

    ì•„ë˜ íŒ¨í„´ë“¤ ì¤‘ í•˜ë‚˜ë¼ë„ target_date ì™€ ê°™ìœ¼ë©´ target_date ë¥¼ ë¦¬í„´, 
    í•˜ë‚˜ë„ ì—†ìœ¼ë©´ None:
    - YYYY-MM-DD
    - MM-DD
    - Mì›” Dì¼ / MMì›” DDì¼
    """

    def _iter_values(x):
        if isinstance(x, dict):
            for v in x.values():
                yield from _iter_values(v)
        elif isinstance(x, list):
            for v in x:
                yield from _iter_values(v)
        else:
            yield x

    texts = [v for v in _iter_values(item) if isinstance(v, str)]

    ty = target_date.year

    # 1) YYYY-MM-DD íŒ¨í„´ë“¤ ì¤‘ì—ì„œ target_date ì™€ ê°™ì€ ë‚ ì§œê°€ ìˆëŠ”ì§€
    for text in texts:
        for yy, mm, dd in re.findall(r"(\d{4})-(\d{2})-(\d{2})", text):
            try:
                dt = date(int(yy), int(mm), int(dd))
            except ValueError:
                continue
            if dt == target_date:
                return dt

    # 2) MM-DD (ì˜ˆ: 12-03)
    for text in texts:
        for mm, dd in re.findall(r"(\d{1,2})-(\d{1,2})", text):
            try:
                dt = date(ty, int(mm), int(dd))
            except ValueError:
                continue
            if dt == target_date:
                return dt

    # 3) '12ì›” 3ì¼' / '12 ì›” 03 ì¼' íŒ¨í„´
    for text in texts:
        for mm, dd in re.findall(r"(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼", text):
            try:
                dt = date(ty, int(mm), int(dd))
            except ValueError:
                continue
            if dt == target_date:
                return dt

    return None

def classify_basketball_volleyball_sport(league: str) -> str:
    """
    mazgtv leagueName ê¸°ì¤€ìœ¼ë¡œ ANALYSIS ì‹œíŠ¸ sport ê°’ì„ ê²°ì •í•œë‹¤.
    - NBA      â†’ "NBA"
    - KBL      â†’ "KBL"
    - WKBL     â†’ "WKBL"
    - V-ë¦¬ê·¸   â†’ "Vë¦¬ê·¸"
    - ê·¸ ì™¸ ë°°êµ¬ ê´€ë ¨ â†’ "ë°°êµ¬"
    - ê·¸ ì™¸ ë†êµ¬ ê´€ë ¨ â†’ "ë†êµ¬"
    """
    if not league:
        return "ë†êµ¬"

    upper = league.upper()

    # NBA
    if "NBA" in upper:
        return "NBA"

    # êµ­ë‚´ ë†êµ¬
    if "KBL" in upper:
        return "KBL"
    if "WKBL" in upper:
        return "WKBL"

    # ë°°êµ¬ (Vë¦¬ê·¸/í•´ì™¸ë°°êµ¬ í¬í•¨)
    if any(x in upper for x in ["V-ë¦¬ê·¸", "Vë¦¬ê·¸", "V-LEAGUE", "VOLLEY", "ë°°êµ¬"]):
        # êµ­ë‚´ Vë¦¬ê·¸ í‘œì‹œë¥¼ ì¡°ê¸ˆ ë” ëª…í™•íˆ í•˜ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸° ë¶„ë¦¬
        if "V" in upper or "V-LEAGUE" in upper:
            return "Vë¦¬ê·¸"
        return "ë°°êµ¬"

    # ë‚˜ë¨¸ì§€ëŠ” ëŒ€ì¶© ë†êµ¬ë¡œ ë¬¶ê¸°
    if any(x in upper for x in ["BASKET", "ë†êµ¬"]):
        return "ë†êµ¬"

    # ì •ë§ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë†êµ¬ë¡œ
    return "ë†êµ¬"

async def crawl_maz_analysis_common(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    *,
    base_url: str,
    sport_label: str,
    league_default: str,
    day_key: str = "tomorrow",
    max_pages: int = 5,
    board_type: int = 2,
    category: int = 1,
    target_ymd: str | None = None,
    export_site: bool = False,
):
    if not is_admin(update):
        await update.message.reply_text("ì´ ëª…ë ¹ì–´ëŠ” ê´€ë¦¬ìë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    # âœ… ë‚ ì§œ ê¸°ì¤€ ì„¤ì • (today/tomorrow)
    if target_ymd is None:
        base_date = get_kst_now().date()
        if day_key == "tomorrow":
            base_date += timedelta(days=1)
        target_ymd = base_date.strftime("%Y-%m-%d")

    target_date = datetime.strptime(target_ymd, "%Y-%m-%d").date()

    await update.message.reply_text(
        f"mazgtv {sport_label} ë¶„ì„ í˜ì´ì§€ì—ì„œ {target_ymd} ê²½ê¸° ë¶„ì„ê¸€ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”..."
    )

    rows_to_append: list[list[str]] = []

    # âœ… ì¤‘ë³µ ë°©ì§€: ì´ë¯¸ today/tomorrow ì‹œíŠ¸ì— ìˆëŠ” src_id ëª¨ìœ¼ê¸°
    existing_ids = get_existing_analysis_ids(day_key)

    try:
        async with httpx.AsyncClient(
            headers={"User-Agent": "Mozilla/5.0"},
            follow_redirects=True,
        ) as client:

            for page in range(1, max_pages + 1):
                list_url = (
                    f"{MAZ_LIST_API}"
                    f"?page={page}&perpage=20"
                    f"&boardType={board_type}&category={category}"
                    f"&sort=b.game_start_at+DESC,+b.created_at+DESC"
                )

                r = await client.get(list_url, timeout=10.0)
                r.raise_for_status()

                try:
                    data = r.json()
                except Exception as e:
                    print(f"[MAZ][LIST] JSON íŒŒì‹± ì‹¤íŒ¨(page={page}): {e}")
                    print("  ì‘ë‹µ ì¼ë¶€:", r.text[:200])
                    continue

                if isinstance(data, dict):
                    items = (
                        data.get("rows")
                        or (data.get("data") or {}).get("rows")
                        or data.get("list")
                        or data.get("items")
                    )
                else:
                    items = data

                if not isinstance(items, list) or not items:
                    print(f"[MAZ][LIST] page={page} í•­ëª© ì—†ìŒ â†’ ë°˜ë³µ ì¢…ë£Œ")
                    break

                for item in items:
                    if not isinstance(item, dict):
                        continue

                    board_id = item.get("id")
                    if not board_id:
                        continue

                    row_id = f"maz_{board_id}"

                    # âœ… ì¤‘ë³µ ìŠ¤í‚µ
                    if row_id in existing_ids:
                        print(f"[MAZ][SKIP_DUP] already exists in sheet: {row_id}")
                        continue

                    game_start_at = (
                        item.get("gameStartAt")
                        or item.get("game_start_at")
                        or ""
                    )
                    game_start_at = str(game_start_at).strip()

                    game_start_at_text = str(item.get("gameStartAtText") or "").strip()
                    print(
                        f"[MAZ][DEBUG] page={page} id={board_id} "
                        f"gameStartAt='{game_start_at}' gameStartAtText='{game_start_at_text}'"
                    )

                    # 1) gameStartAtë¡œ ë‚ ì§œ íŒŒì‹±
                    item_date = _parse_game_start_date(game_start_at)

                    # 2) ì‹¤íŒ¨í•˜ë©´ item ì „ì²´ì—ì„œ ë‚ ì§œ íŒ¨í„´ íƒìƒ‰ (ì—°ë„ ë³´ì •ìš©)
                    if not item_date:
                        item_date = detect_game_date_from_item(item, target_year=target_date.year)

                    print(f"[MAZ][DEBUG_DATE] page={page} id={board_id} item_date={item_date}")

                    if not item_date:
                        continue

                    # âœ… ë‚ ì§œ í•„í„°ë§
                    # - ì¶•êµ¬/ë†êµ¬/ë°°êµ¬: target_dateì™€ ì •í™•íˆ ì¼ì¹˜ë§Œ
                    # - ì•¼êµ¬: (í˜¹ì‹œ ì£¼ê°„ ì¹´ë“œë¡œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš°) ì¼ì¹˜ê°€ ì•„ë‹ˆë©´ ê°™ì€ ì£¼(0~6ì¼)ê¹Œì§€ í—ˆìš©
                    if sport_label == "ì•¼êµ¬":
                        if item_date != target_date:
                            delta_days = (target_date - item_date).days
                            if delta_days < 0 or delta_days >= 7:
                                continue
                    else:
                        if item_date != target_date:
                            continue

                    league = item.get("leagueName") or league_default
                    home = item.get("homeTeamName") or ""
                    away = item.get("awayTeamName") or ""

                    detail_url = MAZ_DETAIL_API_TEMPLATE.format(board_id=board_id)
                    try:
                        r2 = await client.get(detail_url, timeout=10.0)
                        r2.raise_for_status()
                        detail = r2.json()
                    except Exception as e:
                        print(f"[MAZ][DETAIL] id={board_id} ìš”ì²­ ì‹¤íŒ¨: {e}")
                        continue

                    content_html = detail.get("content") or ""
                    if not str(content_html).strip():
                        print(f"[MAZ][DETAIL] id={board_id} content ì—†ìŒ")
                        continue

                    soup = BeautifulSoup(content_html, "html.parser")
                    try:
                        for bad in soup.select("script, style, .ad, .banner"):
                            bad.decompose()
                    except Exception:
                        pass

                    full_text = soup.get_text("\n", strip=True)
                    full_text = clean_maz_text(full_text)
                    if not full_text:
                        print(f"[MAZ][DETAIL] id={board_id} ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì—†ìŒ")
                        continue

                    new_title, new_body = summarize_analysis_with_gemini(
                        full_text,
                        league=league,
                        home_team=home,
                        away_team=away,
                        max_chars=900,
                    )

                    # âœ… sport ì„¸ë¶€ ë¶„ë¥˜
                    row_sport = sport_label

                    if sport_label == "ì¶•êµ¬":
                        if "Kë¦¬ê·¸" in league:
                            row_sport = "Kë¦¬ê·¸"
                        elif "Jë¦¬ê·¸" in league:
                            row_sport = "Jë¦¬ê·¸"
                        else:
                            row_sport = "í•´ì™¸ì¶•êµ¬"

                    elif sport_label == "ì•¼êµ¬":
                        upper_league = (league or "").upper()
                        if "KBO" in upper_league:
                            row_sport = "KBO"
                        elif "NPB" in upper_league:
                            row_sport = "NPB"
                        elif "MLB" in upper_league:
                            row_sport = "í•´ì™¸ì•¼êµ¬"
                        else:
                            row_sport = "í•´ì™¸ì•¼êµ¬"

                    elif sport_label in ("ë†êµ¬", "ë†êµ¬/ë°°êµ¬"):
                        row_sport = classify_basketball_volleyball_sport(league or "")

                    rows_to_append.append([row_sport, row_id, new_title, new_body])

    except Exception as e:
        # âœ… ì—¬ê¸° exceptëŠ” tryì™€ ê°™ì€ ë“¤ì—¬ì“°ê¸° ë ˆë²¨ì´ì–´ì•¼ í•¨
        await update.message.reply_text(f"ìš”ì²­ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return

    if not rows_to_append:
        await update.message.reply_text(
            f"mazgtv {sport_label} ë¶„ì„ì—ì„œ {target_ymd} ê²½ê¸° ë¶„ì„ê¸€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        )
        return

    ok = append_analysis_rows(day_key, rows_to_append)
    if not ok:
        await update.message.reply_text("êµ¬ê¸€ì‹œíŠ¸ì— ë¶„ì„ ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    reload_analysis_from_sheet()

    await update.message.reply_text(
        f"mazgtv {sport_label} ë¶„ì„ì—ì„œ {target_ymd} ê²½ê¸° ë¶„ì„ {len(rows_to_append)}ê±´ì„ "
        f"'{day_key}' ì‹œíŠ¸ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n"
        "í…”ë ˆê·¸ë¨ì—ì„œ ê²½ê¸° ë¶„ì„í”½ ë©”ë‰´ë¥¼ ì—´ì–´ í™•ì¸í•´ë³´ì„¸ìš”."
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¢…ëª©ë³„ (Daum ë‰´ìŠ¤) í¬ë¡¤ë§ ëª…ë ¹ì–´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# í•´ì™¸ì¶•êµ¬
async def crawlsoccer(update: Update, context: ContextTypes.DEFAULT_TYPE):
    cat_id = DAUM_CATEGORY_IDS.get("world_soccer")
    await crawl_daum_news_common(
        update,
        context,
        category_id=cat_id,
        sport_label="ì¶•êµ¬",
        max_articles=5,
    )


# êµ­ë‚´ì¶•êµ¬ (Kë¦¬ê·¸ ë“±, 5ê°œ)
async def crawlsoccerkr(update: Update, context: ContextTypes.DEFAULT_TYPE):
    cat_id = DAUM_CATEGORY_IDS.get("soccer_kleague")
    await crawl_daum_news_common(
        update,
        context,
        category_id=cat_id,
        sport_label="ì¶•êµ¬",   # í•´ì™¸/êµ­ë‚´ë¥¼ í•œ ì¹´í…Œê³ ë¦¬ì— ë¬¶ì–´ì„œ ë³´ì—¬ì£¼ê¸°
        max_articles=5,
    )


# KBO ì•¼êµ¬
async def crawlbaseball(update: Update, context: ContextTypes.DEFAULT_TYPE):
    cat_id = DAUM_CATEGORY_IDS.get("baseball_kbo")
    await crawl_daum_news_common(
        update,
        context,
        category_id=cat_id,
        sport_label="ì•¼êµ¬",
        max_articles=5,
    )


# í•´ì™¸ì•¼êµ¬ (MLB ë“±)
async def crawloverbaseball(update: Update, context: ContextTypes.DEFAULT_TYPE):
    cat_id = DAUM_CATEGORY_IDS.get("baseball_world")
    await crawl_daum_news_common(
        update,
        context,
        category_id=cat_id,
        sport_label="ì•¼êµ¬",  # í•„ìš”í•˜ë©´ 'í•´ì™¸ì•¼êµ¬'ë¡œ ë¶„ë¦¬í•´ì„œë„ ê°€ëŠ¥
        max_articles=5,
    )


# ë†êµ¬
async def crawlbasketball(update: Update, context: ContextTypes.DEFAULT_TYPE):
    cat_id = DAUM_CATEGORY_IDS.get("basketball")
    await crawl_daum_news_common(
        update,
        context,
        category_id=cat_id,
        sport_label="ë†êµ¬",
        max_articles=10,
    )


# ë°°êµ¬
async def crawlvolleyball(update: Update, context: ContextTypes.DEFAULT_TYPE):
    cat_id = DAUM_CATEGORY_IDS.get("volleyball")
    await crawl_daum_news_common(
        update,
        context,
        category_id=cat_id,
        sport_label="ë°°êµ¬",
        max_articles=10,
    )

# 4) ì¸ë¼ì¸ ë²„íŠ¼ ì½œë°± ì²˜ë¦¬ (ë¶„ì„/ë‰´ìŠ¤ íŒì—…)
async def on_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    q = update.callback_query
    data = q.data or ""
    await q.answer()

    # ì•„ë¬´ ë™ì‘ ì•ˆ í•˜ëŠ” ë”ë¯¸
    if data == "noop":
        return

    # ë©”ì¸ ë©”ë‰´ë¡œ
    if data == "back_main":
        await q.edit_message_reply_markup(reply_markup=build_main_inline_menu())
        return

    # ì¶•êµ¬ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ (í•´ì™¸ì¶•êµ¬ / Kë¦¬ê·¸ / Jë¦¬ê·¸)
    if data.startswith("soccer_cat:"):
        _, key, subsport = data.split(":", 2)
        # subsport: "í•´ì™¸ì¶•êµ¬", "Kë¦¬ê·¸", "Jë¦¬ê·¸"
        await q.edit_message_reply_markup(
            reply_markup=build_analysis_match_menu(key, subsport, page=1)
        )
        return

    # ì•¼êµ¬ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ (í•´ì™¸ì•¼êµ¬ / KBO / NPB)
    if data.startswith("baseball_cat:"):
        _, key, subsport = data.split(":", 2)
        # subsport: "í•´ì™¸ì•¼êµ¬", "KBO", "NPB"
        await q.edit_message_reply_markup(
            reply_markup=build_analysis_match_menu(key, subsport, page=1)
        )
        return

        # ë†êµ¬ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ (NBA / KBL)
    if data.startswith("basket_cat:"):
        _, key, subsport = data.split(":", 2)
        # subsport: "NBA", "KBL"
        await q.edit_message_reply_markup(
            reply_markup=build_analysis_match_menu(key, subsport, page=1)
        )
        return

    # ë°°êµ¬ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ (Vë¦¬ê·¸)
    if data.startswith("volley_cat:"):
        _, key, subsport = data.split(":", 2)  # subsport == "Vë¦¬ê·¸"
        await q.edit_message_reply_markup(
            reply_markup=build_analysis_match_menu(key, subsport, page=1)
        )
        return
  
    # ì¢…ëª© ì„ íƒìœ¼ë¡œ ëŒì•„ê°€ê¸°
    if data.startswith("analysis_root:"):
        _, key = data.split(":", 1)
        await q.edit_message_reply_markup(reply_markup=build_analysis_category_menu(key))
        return

    # ì¢…ëª© ì„ íƒ (ì¶•êµ¬/ë†êµ¬/ì•¼êµ¬/ë°°êµ¬)
    if data.startswith("analysis_cat:"):
        _, key, sport = data.split(":", 2)

        # âš½ ì¶•êµ¬ â†’ í•´ì™¸ì¶•êµ¬ / Kë¦¬ê·¸ / Jë¦¬ê·¸ í•˜ìœ„ ë©”ë‰´
        if sport == "ì¶•êµ¬":
            await q.edit_message_reply_markup(
                reply_markup=build_soccer_subcategory_menu(key)
            )
            return

        # âš¾ ì•¼êµ¬ â†’ í•´ì™¸ì•¼êµ¬ / KBO / NPB í•˜ìœ„ ë©”ë‰´
        if sport == "ì•¼êµ¬":
            await q.edit_message_reply_markup(
                reply_markup=build_baseball_subcategory_menu(key)
            )
            return

        # ğŸ€ ë†êµ¬ â†’ NBA / KBL í•˜ìœ„ ë©”ë‰´
        if sport == "ë†êµ¬":
            await q.edit_message_reply_markup(
                reply_markup=build_basketball_subcategory_menu(key)
            )
            return

        # ğŸ ë°°êµ¬ â†’ Vë¦¬ê·¸ í•˜ìœ„ ë©”ë‰´
        if sport == "ë°°êµ¬":
            await q.edit_message_reply_markup(
                reply_markup=build_volleyball_subcategory_menu(key)
            )
            return        

        # ê·¸ ì™¸ ì¢…ëª©(ë°°êµ¬ ë“±)ì€ ë°”ë¡œ ê²½ê¸° ë¦¬ìŠ¤íŠ¸ 1í˜ì´ì§€
        await q.edit_message_reply_markup(
            reply_markup=build_analysis_match_menu(key, sport, page=1)
        )
        return
        
    # ê²½ê¸° ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ì´ë™ (ì´ì „/ë‹¤ìŒ)
    if data.startswith("match_page:"):
        _, key, sport, page_str = data.split(":", 3)
        try:
            page = int(page_str)
        except ValueError:
            page = 1

        await q.edit_message_reply_markup(
            reply_markup=build_analysis_match_menu(key, sport, page=page)
        )
        return

    # ê°œë³„ ê²½ê¸° ì„ íƒ
    if data.startswith("match:"):
        _, key, sport, match_id = data.split(":", 3)
        items = ANALYSIS_DATA_MAP.get(key, {}).get(sport, [])

        title = "ì„ íƒí•œ ê²½ê¸°"
        summary = "í•´ë‹¹ ê²½ê¸° ë¶„ì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        for item in items:
            if item["id"] == match_id:
                title = item["title"]
                summary = item["summary"]
                break

        text = f"ğŸ“Œ ê²½ê¸° ë¶„ì„ â€“ {title}\n\n{summary}"

        buttons = [
            [InlineKeyboardButton("ğŸ“º ìŠ¤í¬ì¸  ë¬´ë£Œ ì¤‘ê³„", url="https://goat-tv.com")],
            [InlineKeyboardButton("ğŸ“ ë¶„ì„ê¸€ ë” ë³´ê¸°", callback_data=f"analysis_root:{key}")],
            [InlineKeyboardButton("â—€ ë©”ì¸ ë©”ë‰´ë¡œ", callback_data="back_main")],
        ]

        await q.message.reply_text(text, reply_markup=InlineKeyboardMarkup(buttons))
        return

    # ë‰´ìŠ¤ ë£¨íŠ¸
    if data == "news_root":
        await q.edit_message_reply_markup(reply_markup=build_news_category_menu())
        return

    # ë‰´ìŠ¤ ì¢…ëª© ì„ íƒ
    if data.startswith("news_cat:"):
        sport = data.split(":", 1)[1]
        await q.edit_message_reply_markup(reply_markup=build_news_list_menu(sport))
        return

    # ë‰´ìŠ¤ ì•„ì´í…œ ì„ íƒ
    if data.startswith("news_item:"):
        try:
            _, sport, news_id = data.split(":", 2)
            items = NEWS_DATA.get(sport, [])
            title = "ë‰´ìŠ¤ ì •ë³´ ì—†ìŒ"
            summary = "í•´ë‹¹ ë‰´ìŠ¤ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            for item in items:
                if item["id"] == news_id:
                    title = item["title"]
                    summary = item["summary"]
                    break
        except Exception:
            title = "ë‰´ìŠ¤ ì •ë³´ ì—†ìŒ"
            summary = "í•´ë‹¹ ë‰´ìŠ¤ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        text = f"ğŸ“° ë‰´ìŠ¤ ìš”ì•½ â€“ {title}\n\n{summary}"

        buttons = [
            [InlineKeyboardButton("ğŸ“º ìŠ¤í¬ì¸ ë¬´ë£Œì¤‘ê³„", url="https://goat-tv.com")],
            [InlineKeyboardButton("ğŸ“° ë‹¤ë¥¸ ë‰´ìŠ¤ ë³´ê¸°", callback_data="news_root")],
            [InlineKeyboardButton("â—€ ë©”ì¸ ë©”ë‰´ë¡œ", callback_data="back_main")],
        ]

        await q.message.reply_text(
            text,
            reply_markup=InlineKeyboardMarkup(buttons),
        )
        return

async def crawlmazsoccer_tomorrow(update: Update, context: ContextTypes.DEFAULT_TYPE):
    # 1) í•´ì™¸ì¶•êµ¬
    await crawl_maz_analysis_common(
        update,
        context,
        base_url="https://mazgtv1.com/analyze/overseas",
        sport_label="ì¶•êµ¬",
        league_default="í•´ì™¸ì¶•êµ¬",
        day_key="tomorrow",
        max_pages=5,
        board_type=2,
        category=1,
        export_site=True,   # âœ… ì¶”ê°€
    )

    # 2) Kë¦¬ê·¸/Jë¦¬ê·¸(asia)
    await crawl_maz_analysis_common(
        update,
        context,
        base_url="https://mazgtv1.com/analyze/asia",
        sport_label="ì¶•êµ¬",
        league_default="Kë¦¬ê·¸/Jë¦¬ê·¸",
        day_key="tomorrow",
        max_pages=5,
        board_type=2,
        category=2,
        export_site=True,   # âœ… ì¶”ê°€
    )

    await update.message.reply_text("âš½ í…”ë ˆê·¸ë¨ìš© + ì‚¬ì´íŠ¸ìš©(ë‚´ì¼) ë¶„ì„ í¬ë¡¤ë§ì„ ëª¨ë‘ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


# ì•¼êµ¬(MLB Â· KBO Â· NPB) ë¶„ì„ (ë‚´ì¼ ê²½ê¸° â†’ tomorrow ì‹œíŠ¸)
async def crawlmazbaseball_tomorrow(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    mazgtv ì•¼êµ¬(MLB / KBO / NPB) ë‚´ì¼ ê²½ê¸° ë¶„ì„ì„ í¬ë¡¤ë§í•´ì„œ
    'tomorrow' ì‹œíŠ¸ì— ì €ì¥í•œë‹¤. ì¶•êµ¬ìš©ê³¼ ë™ì¼í•œ êµ¬ì¡°.
    """
    # í•´ì™¸ì•¼êµ¬(MLB)
    await crawl_maz_analysis_common(
        update,
        context,
        base_url="https://mazgtv1.com/analyze/mlb",
        sport_label="ì•¼êµ¬",
        league_default="í•´ì™¸ì•¼êµ¬",
        day_key="tomorrow",
        max_pages=5,
        board_type=2,
        category=3,
    )

    # KBO + NPB
    await crawl_maz_analysis_common(
        update,
        context,
        base_url="https://mazgtv1.com/analyze/baseball",
        sport_label="ì•¼êµ¬",
        league_default="KBO/NPB",
        day_key="tomorrow",
        max_pages=5,
        board_type=2,
        category=4,
    )

    await update.message.reply_text(
        "âš¾ ì•¼êµ¬(MLB Â· KBO Â· NPB) ë‚´ì¼ ê²½ê¸° ë¶„ì„ í¬ë¡¤ë§ ëª…ë ¹ì„ ëª¨ë‘ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤."
    )

# ğŸ”¹ NBA + êµ­ë‚´ ë†êµ¬/ë°°êµ¬ (ë‚´ì¼ ê²½ê¸°) í¬ë¡¤ë§
async def bvcrawl_tomorrow(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    mazgtv ë†êµ¬/ë°°êµ¬ ë¶„ì„:
    - NBA ë¶„ì„:    https://mazgtv1.com/analyze/nba
    - êµ­ë‚´ ë†êµ¬/ë°°êµ¬: https://mazgtv1.com/analyze/volleyball
    ë‘ ê³³ì—ì„œ 'ë‚´ì¼ ê²½ê¸°' ë¶„ì„ê¸€ì„ í¬ë¡¤ë§í•´ì„œ tomorrow ì‹œíŠ¸ì— ì €ì¥í•œë‹¤.
    """

    # 1) NBA (í•´ì™¸ ë†êµ¬)
    await crawl_maz_analysis_common(
        update,
        context,
        base_url="https://mazgtv1.com/analyze/nba",
        sport_label="ë†êµ¬",          # ì‹œíŠ¸ì—ëŠ” NBA/KBL/WKBL ë“±ìœ¼ë¡œ ë‚˜ë‰¨
        league_default="NBA",
        day_key="tomorrow",
        max_pages=5,
        board_type=2,                # âš ï¸ ì‹¤ì œ boardType ê°’ìœ¼ë¡œ ìˆ˜ì • í•„ìš”
        category=5,                  # âš ï¸ ì‹¤ì œ category ê°’ìœ¼ë¡œ ìˆ˜ì • í•„ìš”
        # target_ymd=None â†’ ìë™ìœ¼ë¡œ 'ë‚´ì¼' ë‚ ì§œ ì‚¬ìš©
    )

    # 2) êµ­ë‚´ ë†êµ¬ + ë°°êµ¬ (KBL / WKBL / Vë¦¬ê·¸ ë“±)
    await crawl_maz_analysis_common(
        update,
        context,
        base_url="https://mazgtv1.com/analyze/volleyball",
        sport_label="ë†êµ¬/ë°°êµ¬",     # ë¶„ë¥˜ í•¨ìˆ˜ì—ì„œ KBL/WKBL/Vë¦¬ê·¸/ë°°êµ¬ ë“±ìœ¼ë¡œ ì„¸ë¶„í™”
        league_default="êµ­ë‚´ë†êµ¬/ë°°êµ¬",
        day_key="tomorrow",
        max_pages=5,
        board_type=2,                # âš ï¸ ì‹¤ì œ boardType ê°’ìœ¼ë¡œ ìˆ˜ì • í•„ìš”
        category=7,                  # âš ï¸ ì‹¤ì œ category ê°’ìœ¼ë¡œ ìˆ˜ì • í•„ìš”
    )

    await update.message.reply_text(
        "NBA + êµ­ë‚´ ë†êµ¬/ë°°êµ¬(ë‚´ì¼ ê²½ê¸°) ë¶„ì„ í¬ë¡¤ë§ì„ ëª¨ë‘ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤.\n"
        "/syncsheet ë¡œ í…”ë ˆê·¸ë¨ ë©”ë‰´ ë°ì´í„°ë¥¼ ê°±ì‹ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

async def crawlmazsoccer_today(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    mazgtv í•´ì™¸ì¶•êµ¬ + Kë¦¬ê·¸/Jë¦¬ê·¸ ë¶„ì„ ì¤‘
    'ì˜¤ëŠ˜ ë‚ ì§œ' ê²½ê¸°ë¥¼ í¬ë¡¤ë§í•´ì„œ today ì‹œíŠ¸ì— ì €ì¥.
    """

    # 1) í•´ì™¸ì¶•êµ¬ íƒ­
    await crawl_maz_analysis_common(
        update,
        context,
        base_url="https://mazgtv1.com/analyze/overseas",
        sport_label="ì¶•êµ¬",          # ì•ˆì—ì„œ 'í•´ì™¸ì¶•êµ¬/Kë¦¬ê·¸/Jë¦¬ê·¸'ë¡œ ë‹¤ì‹œ ë¶„ë¥˜ë¨
        league_default="í•´ì™¸ì¶•êµ¬",
        day_key="today",            # âœ… today
        max_pages=5,
        board_type=2,
        category=1,                 # í•´ì™¸ì¶•êµ¬
    )

    # 2) Kë¦¬ê·¸ / Jë¦¬ê·¸ íƒ­
    await crawl_maz_analysis_common(
        update,
        context,
        base_url="https://mazgtv1.com/analyze/asia",
        sport_label="ì¶•êµ¬",
        league_default="Kë¦¬ê·¸/Jë¦¬ê·¸",
        day_key="today",            # âœ… today
        max_pages=5,
        board_type=2,
        category=2,                 # Kë¦¬ê·¸/Jë¦¬ê·¸
    )

    await update.message.reply_text(
        "âš½ í•´ì™¸ì¶•êµ¬ + Kë¦¬ê·¸/Jë¦¬ê·¸ ì˜¤ëŠ˜ ê²½ê¸° ë¶„ì„ í¬ë¡¤ë§ì„ ëª¨ë‘ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤."
    )

async def crawlmazbaseball_today(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    mazgtv ì•¼êµ¬ ë¶„ì„(MLB + KBO + NPB) ì¤‘
    'ì˜¤ëŠ˜ ë‚ ì§œ' ê²½ê¸°ë¥¼ í¬ë¡¤ë§í•´ì„œ today ì‹œíŠ¸ì— ì €ì¥.
    """

    # 1) í•´ì™¸ì•¼êµ¬ (MLB)
    await crawl_maz_analysis_common(
        update,
        context,
        base_url="https://mazgtv1.com/analyze/mlb",
        sport_label="ì•¼êµ¬",          # ì‹œíŠ¸ì—ì„œëŠ” í•´ì™¸ì•¼êµ¬/KBO/NPBë¡œ ë¶„ë¦¬ë¨
        league_default="í•´ì™¸ì•¼êµ¬",
        day_key="today",            # ğŸ”´ ì˜¤ëŠ˜
        max_pages=5,
        board_type=2,               # ê¸°ì¡´ /crawlmazbaseball_tomorrow ì™€ ë™ì¼
        category=3,                 # MLB ìª½ category ê°’ (ì§€ê¸ˆ ì“°ëŠ” ê°’ ê·¸ëŒ€ë¡œ)
    )

    # 2) KBO + NPB
    await crawl_maz_analysis_common(
        update,
        context,
        base_url="https://mazgtv1.com/analyze/baseball",
        sport_label="ì•¼êµ¬",
        league_default="KBO/NPB",
        day_key="today",            # ğŸ”´ ì˜¤ëŠ˜
        max_pages=5,
        board_type=2,               # ë™ì¼ boardType
        category=4,                 # KBO/NPB ìª½ category ê°’ (ì§€ê¸ˆ ì“°ëŠ” ê°’ ê·¸ëŒ€ë¡œ)
    )

    await update.message.reply_text(
        "âš¾ mazgtv ì•¼êµ¬(MLB Â· KBO Â· NPB) 'ì˜¤ëŠ˜ ê²½ê¸°' ë¶„ì„ í¬ë¡¤ë§ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.\n"
        "today ì‹œíŠ¸ì—ì„œ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

# ğŸ”¹ NBA + êµ­ë‚´ ë†êµ¬/ë°°êµ¬ (ì˜¤ëŠ˜ ê²½ê¸°) í¬ë¡¤ë§
async def bvcrawl_today(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    mazgtv ë†êµ¬/ë°°êµ¬ ë¶„ì„:
    - NBA ë¶„ì„:    https://mazgtv1.com/analyze/nba
    - êµ­ë‚´ ë†êµ¬/ë°°êµ¬: https://mazgtv1.com/analyze/volleyball
    ë‘ ê³³ì—ì„œ 'ì˜¤ëŠ˜ ê²½ê¸°' ë¶„ì„ê¸€ì„ í¬ë¡¤ë§í•´ì„œ today ì‹œíŠ¸ì— ì €ì¥í•œë‹¤.
    """

    # 1) NBA (í•´ì™¸ ë†êµ¬)
    await crawl_maz_analysis_common(
        update,
        context,
        base_url="https://mazgtv1.com/analyze/nba",
        sport_label="ë†êµ¬",
        league_default="NBA",
        day_key="today",             # âœ… ì˜¤ëŠ˜
        max_pages=5,
        board_type=2,                # ğŸ‘‰ tomorrowì™€ ë™ì¼ ê°’ ìœ ì§€
        category=5,
    )

    # 2) êµ­ë‚´ ë†êµ¬ + ë°°êµ¬ (KBL / WKBL / Vë¦¬ê·¸ ë“±)
    await crawl_maz_analysis_common(
        update,
        context,
        base_url="https://mazgtv1.com/analyze/volleyball",
        sport_label="ë†êµ¬/ë°°êµ¬",
        league_default="êµ­ë‚´ë†êµ¬/ë°°êµ¬",
        day_key="today",             # âœ… ì˜¤ëŠ˜
        max_pages=5,
        board_type=2,                # ğŸ‘‰ tomorrowì™€ ë™ì¼ ê°’ ìœ ì§€
        category=7,
    )

    await update.message.reply_text(
        "NBA + êµ­ë‚´ ë†êµ¬/ë°°êµ¬(ì˜¤ëŠ˜ ê²½ê¸°) ë¶„ì„ í¬ë¡¤ë§ì„ ëª¨ë‘ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤.\n"
        "today ì‹œíŠ¸ì—ì„œ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹¤í–‰ë¶€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    reload_analysis_from_sheet()
    reload_news_from_sheet()

    app = ApplicationBuilder().token(TOKEN).build()

    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("myid", myid))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, on_text))

    app.add_handler(CommandHandler("publish", publish))
    app.add_handler(CommandHandler("syncsheet", syncsheet))
    # ë‰´ìŠ¤ ì‹œíŠ¸ ì „ì²´ ì´ˆê¸°í™”
    app.add_handler(CommandHandler("newsclean", newsclean))
    # today / tomorrow / news ì „ì²´ ì´ˆê¸°í™”
    app.add_handler(CommandHandler("allclean", allclean))    

    # ë¶„ì„ ì‹œíŠ¸ ë¶€ë¶„ ì´ˆê¸°í™” ëª…ë ¹ì–´ë“¤ (ëª¨ë‘ tomorrow ì‹œíŠ¸ ê¸°ì¤€)
    app.add_handler(CommandHandler("soccerclean", soccerclean))
    app.add_handler(CommandHandler("baseballclean", baseballclean))
    app.add_handler(CommandHandler("basketclean", basketclean))
    app.add_handler(CommandHandler("volleyclean", volleyclean))
    app.add_handler(CommandHandler("etcclean", etcclean))
    app.add_handler(CommandHandler("analysisclean", analysisclean))

    app.add_handler(CommandHandler("rollover", rollover))

    # ë‰´ìŠ¤ í¬ë¡¤ë§ ëª…ë ¹ì–´ë“¤ (Daum)
    app.add_handler(CommandHandler("crawlsoccer", crawlsoccer))             # í•´ì™¸ì¶•êµ¬
    app.add_handler(CommandHandler("crawlsoccerkr", crawlsoccerkr))         # êµ­ë‚´ì¶•êµ¬
    app.add_handler(CommandHandler("crawlbaseball", crawlbaseball))         # KBO
    app.add_handler(CommandHandler("crawloverbaseball", crawloverbaseball)) # í•´ì™¸ì•¼êµ¬
    app.add_handler(CommandHandler("crawlbasketball", crawlbasketball))     # ë†êµ¬
    app.add_handler(CommandHandler("crawlvolleyball", crawlvolleyball))     # ë°°êµ¬

    # mazgtv í•´ì™¸ì¶•êµ¬ ë¶„ì„ (ì˜¤ëŠ˜ / ë‚´ì¼ ê²½ê¸° â†’ today / tomorrow ì‹œíŠ¸)
    app.add_handler(CommandHandler("crawlmazsoccer_today", crawlmazsoccer_today))
    app.add_handler(CommandHandler("crawlmazsoccer_tomorrow", crawlmazsoccer_tomorrow))

    # mazgtv ì•¼êµ¬ ë¶„ì„ (ì˜¤ëŠ˜ / ë‚´ì¼)
    app.add_handler(CommandHandler("crawlmazbaseball_today", crawlmazbaseball_today))
    app.add_handler(CommandHandler("crawlmazbaseball_tomorrow", crawlmazbaseball_tomorrow))

    # mazgtv ë†êµ¬ + ë°°êµ¬ ë¶„ì„ (ì˜¤ëŠ˜ / ë‚´ì¼)
    app.add_handler(CommandHandler("bvcrawl_today", bvcrawl_today))
    app.add_handler(CommandHandler("bvcrawl_tomorrow", bvcrawl_tomorrow))




    app.add_handler(CallbackQueryHandler(on_callback))

    port = int(os.environ.get("PORT", "10000"))
    app.run_webhook(
        listen="0.0.0.0",
        port=port,
        url_path=TOKEN,
        webhook_url=f"{APP_URL}/{TOKEN}",
    )


if __name__ == "__main__":
    main()
















